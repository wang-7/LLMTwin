{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37be023",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedeb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "from llmtwin.domain.cleaned_document import CleanedArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8291dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_doc, _ = CleanedArticle.bulk_find(limit=999, with_payloads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17cbf029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026f413e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CleanedArticle(id=UUID('009c1af0-42e7-47c6-9205-6e56df462af5'), content='DML 7 steps to build a production ready financial assistant using LLMs How to fine tune any LLM at scale in under 5 minutes. 7 steps to build a production ready financial assistant using LLMs. SubscribeSign in Share this post DML 7 steps to build a production ready financial assistant using LLMs decodingml.substack.com Copy link Facebook Email Note Other DML 7 steps to build a production ready financial assistant using LLMs How to fine tune any LLM at scale in under 5 minutes. 7 steps to build a production ready financial assistant using LLMs. Paul Iusztin Oct 12, 2023 5 Share this post DML 7 steps to build a production ready financial assistant using LLMs decodingml.substack.com Copy link Facebook Email Note Other Share _Hello there, I am Paul Iusztin _ _Within this newsletter, I will help you decode complex topics about ML MLOps one week at a time _ This week s ML MLOps topics 1. Writing your own ML models is history. How to fine tune any LLM at scale in under 5 minutes. 2. 7 steps to chain your prompts to build a production ready financial assistant using LLMs. Extra 3 key resources on how to monitor your ML models 1. Writing your own ML models is history. How to fine tune any LLM at scale in under 5 minutes. Writing your own ML models is history. The true value is in your data, how you prepare it, and your computer power. To demonstrate my statement. Here is how you can write a Python script to train your LLM at scale in under 5 minutes ğŸ­. Load your data in JSON format and convert it into a Hugging Dataset ğŸ®. Use Huggingface to load the LLM and pass it to the SFTTrainer, along with the tokenizer and training evaluation datasets. ğŸ¯. Wrap your training script with a serverless solution, such as Beam, which quickly lets you access a cluster of GPUs to train large models. As you can see, the secret ingredients are not the LLM but the amount of data the quality of data how you process the data for compute power the ability to scale the system 3 steps to write a Python script to train your LLMs at scale Image by the Author . My advice If you don t plan to become an ML researcher, shift your focus from the latest models to your data and infrastructure. . ğ—¡ğ—¼ğ˜ğ—² Integrating serverless services, such as Beam, makes the deployment of your training pipeline fast seamless, leaving you to focus only on the last piece of the puzzle your data. Check out Beam s docs to find out more. 2. 7 steps to chain your prompts to build a production ready financial assistant using LLMs. ğŸ³ ğ˜€ğ˜ğ—²ğ—½ğ˜€ on how to ğ—°ğ—µğ—®ğ—¶ğ—» your ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€ to build a production ready ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜ using ğ—Ÿğ—Ÿğ— ğ˜€ When building LLM applications, you frequently have to divide your application into multiple steps prompts, which are known as chaining prompts . Here are 7 standard steps when building a financial assistant using LLMs or any other assistant ğ—¦ğ˜ğ—²ğ—½ ğŸ­ Check if the user s question is safe using OpenAI s Moderation API If the user s query is safe, move to ğ—¦ğ˜ğ—²ğ—½ ğŸ® ğ—¦ğ˜ğ—²ğ—½ ğŸ® Query your proprietary data e.g., financial news to enrich the prompt with fresh data additional context. To do so, you have to use an LM to embed the user s input use the embedding to query your proprietary data stored in a vector DB ğ˜•ğ˜°ğ˜µğ˜¦ You must use the same LM model to embed the data that will be stored in the vector DB the user s question used to query the vector DB ğ—¦ğ˜ğ—²ğ—½ ğŸ¯ Build the prompt using a predefined template the user s question extracted financial news as context your conversation history as context ğ—¦ğ˜ğ—²ğ—½ ğŸ° Call the LLM ğ—¦ğ˜ğ—²ğ—½ ğŸ± Check if the assistant s answer is safe using the OpenAI s Moderation API. If the assistant s answer is safe, move to ğ—¦ğ˜ğ—²ğ—½ ğŸ± ğ—¦ğ˜ğ—²ğ—½ ğŸ² Use an LLM to check if the final answer is satisfactory. To do so, you build a prompt using the following a validation predefined template the user s initial question the assistants answer The LLM has to give a yes or no answer. Thus, if it answers yes, we show the final answer to the user. Otherwise, we will return a predefined response, such as Sorry, we couldn t answer your question because we don t have enough information. ğ—¦ğ˜ğ—²ğ—½ ğŸ³ Add the user s question and assistant s answer to a history cache. Which will be used to enrich the following prompts with the current conversation. Just to remind you, the assistant should support a conversation. Thus, it needs to know what happened in the previous questions. In practice, you usually keep only the latest N question, answer tuples or a conversation summary to keep your context length under control. 7 Steps to Build a Production Ready Financial Assistant Using LLMs Image by the Author If you want to see this strategy in action, check out our new FREE Hands on LLMs course work in progress give it a on GitHub to stay updated with its latest progress. Extra 3 key resources on how to monitor your ML models In the last month, I read 100 ML monitoring articles. I trimmed them for you to 3 key resources 1 . A series of excellent articles made by Arize AI that will make you understand what ML monitoring is all about. Arize Articles 2 . The Evidently AI Blog, where you can find answers to all your questions regarding ML monitoring. Evidently Blog 3 . The monitoring hands on examples hosted by DataTalksClub will teach you how to implement an ML monitoring system. DataTalks Course After wasting a lot of time reading other resources... Using these 3 resources is a solid start for learning about monitoring ML systems. That s it for today See you next Thursday at 9 00 a.m. CET. Have a fantastic weekend! Paul Whenever you re ready, here is how I can help you 1. The Full Stack 7 Steps MLOps Framework a 7 lesson FREE course that will walk you step by step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code 2.5 hours of reading video materials on Medium. 2. Machine Learning MLOps Blog in depth topics about designing and productionizing ML systems using MLOps. 3. Machine Learning MLOps Hub a place where all my work is aggregated in one place courses, articles, webinars, podcasts, etc. . 5 Share this post DML 7 steps to build a production ready financial assistant using LLMs decodingml.substack.com Copy link Facebook Email Note Other Share PreviousNext Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Note Other This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en', platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', link='https://decodingml.substack.com/p/dml-7-steps-to-build-a-production?r=1ttoeh')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7688c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/a100/data/WQ/private/LLMTwin/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "\u001b[32m2026-01-02 21:17:29.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmtwin.preprocess.dispatcher\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mDocument chunked successfully.\u001b[0m\n",
      "\u001b[32m2026-01-02 21:17:29.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmtwin.preprocess.dispatcher\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mDocument chunked successfully.\u001b[0m\n",
      "\u001b[32m2026-01-02 21:17:30.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmtwin.preprocess.dispatcher\u001b[0m:\u001b[36mdispatch\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mData embedded successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llmtwin.feature_engineering.chunk_and_embed import chunk_and_embed\n",
    "\n",
    "result = chunk_and_embed(cleaned_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b3eaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EmbedArticle(id=UUID('be3cda52-7648-4a23-b4cb-c3cfed18ca59'), content='DML 7 steps to build a production ready financial assistant using LLMs How to fine tune any LLM at scale in under 5 minutes. 7 steps to build a production ready financial assistant using LLMs. SubscribeSign in Share this post DML 7 steps to build a production ready financial assistant using LLMs decodingml.substack.com Copy link Facebook Email Note Other DML 7 steps to build a production ready financial assistant using LLMs How to fine tune any LLM at scale in under 5 minutes. 7 steps to build a production ready financial assistant using LLMs. Paul Iusztin Oct 12, 2023 5 Share this post DML 7 steps to build a production ready financial assistant using LLMs decodingml.substack.com Copy link Facebook Email Note Other Share _Hello there, I am Paul Iusztin _ _Within this newsletter, I will help you decode complex topics about ML MLOps one week at a time _ This week s ML MLOps topics 1. Writing your own ML models is history. How to fine tune any LLM at scale in under 5 minutes. 2. 7 steps to chain your prompts to build a production ready financial assistant using LLMs. Extra 3 key resources on how to monitor your ML models 1. Writing your own ML models is history. How to fine tune any LLM at scale in under 5 minutes. Writing your own ML models is history. The true value is in your data, how you prepare it, and your computer power. To demonstrate my statement. Here is how you can write a Python script to train your LLM at scale in under 5 minutes ğŸ­. Load your data in JSON format and convert it into a Hugging Dataset ğŸ®. Use Huggingface to load the LLM and pass it to the SFTTrainer, along with the tokenizer and training evaluation datasets. ğŸ¯. Wrap your training script with a serverless solution, such as Beam, which quickly lets you access a cluster of GPUs to train large models.', embedding=[0.0027300789952278137, -0.04067410156130791, 0.004897721577435732, 0.033565979450941086, -0.004271044861525297, -0.03896186873316765, -0.07335814088582993, 0.01908973976969719, -0.06152263283729553, 0.03873618319630623, -0.03631509095430374, -0.06234005093574524, -0.014675252139568329, -0.07376156747341156, -0.010912738740444183, 0.029842590913176537, 0.06741322576999664, -0.013536444865167141, -0.1294020414352417, -0.02620210498571396, 0.06102199852466583, -0.07361564040184021, 0.0182440597563982, -0.01534054335206747, -0.0038218479603528976, -0.0439542755484581, 0.059841230511665344, 0.05420159175992012, 0.005544526502490044, -0.05527309700846672, 0.024280963465571404, 0.04050355404615402, 0.07718774676322937, 0.048883017152547836, 0.012414833530783653, 0.10362367331981659, 0.03520212695002556, 0.047463446855545044, 0.0030828705057501793, -0.034802377223968506, -0.0042975470423698425, -0.06045127660036087, -0.033370885998010635, -0.02886572666466236, 0.04915115237236023, -0.04217671602964401, -0.004003438632935286, -0.08122692257165909, -0.05458119511604309, -0.034014761447906494, -0.08235810697078705, -0.023369891569018364, -0.017075354233384132, -0.04798786714673042, -0.06342596560716629, 0.06113092601299286, 0.03999258205294609, -0.022536182776093483, -0.08870185166597366, -0.08333681523799896, -0.06659269332885742, -0.03688240796327591, -0.051195625215768814, -0.02244560793042183, 0.019858956336975098, 0.046188920736312866, 0.024684110656380653, 0.06302817910909653, 0.02558514103293419, -0.0016579607035964727, -0.03563474491238594, -0.051892202347517014, -0.08268146961927414, 0.022851532325148582, -0.025981690734624863, 0.01059748325496912, 0.04541058465838432, 0.009056801907718182, 0.057755425572395325, -0.08726091682910919, -0.01194057334214449, 0.07329830527305603, -0.02399044670164585, -0.008582632057368755, -0.0629943385720253, -0.016908669844269753, 0.026942884549498558, 0.04542090371251106, 0.09016773104667664, 0.027310317382216454, 0.10135869681835175, -0.005204776301980019, -0.03309297934174538, -0.027092212811112404, 0.0883907601237297, 0.0386536680161953, 0.010366348549723625, -0.06709935516119003, 0.02592102438211441, 0.0032237335108220577, 0.031370628625154495, 0.06832478940486908, 0.04672974348068237, -0.006601925473660231, -0.11593151837587357, -0.008695162832736969, 0.024411430582404137, 0.13697494566440582, -0.027118604630231857, -0.09811869263648987, 0.04267215356230736, 0.06777076423168182, 0.025306018069386482, -0.054983895272016525, 0.03650804981589317, 0.01784299686551094, 0.002132402267307043, -0.09236184507608414, 0.03878944739699364, 0.08415895700454712, 0.008657868951559067, 0.023637913167476654, -0.016288628801703453, -0.01960844360291958, -0.035794444382190704, -0.02805178612470627, -0.07267563045024872, 1.38675728122474e-32, 0.041717998683452606, 0.05776195973157883, -0.011734843254089355, 0.035841990262269974, -0.0050087980926036835, 0.05053401365876198, 0.03484411910176277, 0.05515662580728531, -0.07267643511295319, 0.044356875121593475, -0.04185562953352928, 0.011516011320054531, -0.06577183306217194, -0.03186716511845589, -0.057791370898485184, -0.09199842065572739, 0.045097023248672485, 0.07831669598817825, -0.02922476828098297, 0.020122362300753593, 0.08302390575408936, -0.0789300948381424, 0.029828447848558426, -0.1134224534034729, 0.11637727171182632, 0.0723150447010994, 0.058891117572784424, -0.0006787884049117565, 0.0017255856655538082, 0.016093198210000992, -0.02888643927872181, -0.041357118636369705, -0.057720739394426346, -0.008684615604579449, 0.038349222391843796, -0.012603318318724632, -0.033542219549417496, -0.015699176117777824, 0.028556568548083305, -0.07805947214365005, -0.04673805832862854, 0.009864806197583675, 0.015734434127807617, -0.07597222924232483, -0.1094026118516922, 0.012788517400622368, -0.012368441559374332, -0.03276088833808899, 0.050692372024059296, 0.016150083392858505, 0.08616550266742706, -0.01410655491054058, -0.059579744935035706, -0.0850525051355362, -0.04857254400849342, 0.03408654406666756, 0.004950201138854027, -0.06184756010770798, -0.02108517661690712, 0.032588012516498566, 0.03301513195037842, -0.013283228501677513, -0.03262878954410553, 0.05627717822790146, 0.017816398292779922, 0.0006051678792573512, -0.06073915213346481, 0.07391851395368576, 0.06585700809955597, -0.07369072735309601, -0.05531066283583641, 0.002377915196120739, 0.05021767318248749, -0.03198279067873955, 0.06862737983465195, -0.04695214331150055, 0.050348538905382156, -0.03611297160387039, 0.012976309284567833, 0.023661572486162186, 0.06057319417595863, 0.08265563100576401, -0.05656207352876663, -0.020439503714442253, 0.09192196279764175, -0.018593652173876762, 0.0017308598617091775, -0.06779751926660538, 0.023786593228578568, 0.01825917325913906, -0.05418332666158676, -0.0352226123213768, 0.00013613745977636427, 0.0485028475522995, -0.0050817085430026054, -1.355001154400482e-32, 0.011492118239402771, -0.05349794402718544, -0.0301832128316164, 0.06710357218980789, -0.001710850978270173, -0.0253624077886343, -0.05241236463189125, -0.015823371708393097, 0.009703999385237694, -0.0007139751687645912, -0.10185494273900986, -0.058872368186712265, -0.055498670786619186, -0.07990472763776779, -0.059327080845832825, -0.06118009611964226, 0.011635111644864082, -0.06936541944742203, 0.0773673951625824, 0.08003143966197968, -0.015820369124412537, 0.036483652889728546, -0.07875355333089828, 0.008860796689987183, -0.018981559202075005, 0.03392859548330307, -0.034080084413290024, 0.1130165085196495, 0.021831907331943512, 0.026139236986637115, 0.008154203183948994, -0.008945504203438759, 0.019342217594385147, 0.0031014771666377783, -0.07773775607347488, -0.04919443279504776, 0.03985346853733063, 0.012286355718970299, 0.035591259598731995, 0.058941956609487534, 0.13137243688106537, 0.0022382016759365797, -0.04312876611948013, -0.07776881009340286, -0.01664697378873825, -0.08473405241966248, -0.033058445900678635, -0.06962991505861282, 0.02166147716343403, 0.017910677939653397, 0.0023677118588238955, -0.022808538749814034, -0.04516036435961723, -0.0017171900253742933, -0.0343533493578434, -0.024023471400141716, 0.06672582775354385, -0.03860105201601982, -0.01197625882923603, 0.024622660130262375, -0.10966695100069046, 0.014045659452676773, 0.09035436064004898, -0.0371231734752655, 0.04289814084768295, -0.025818925350904465, -0.03943359851837158, -0.027369560673832893, -0.0821465253829956, -0.06398366391658783, -0.05763658508658409, -0.06625160574913025, -0.03187667205929756, -0.007648867554962635, 0.07117626816034317, -0.06049192324280739, -0.07967698574066162, -0.0809243693947792, 0.04294585436582565, 0.004865147639065981, -0.0042962501756846905, 0.010432643815875053, -0.019995639100670815, 0.07947150617837906, -0.03379248455166817, 0.013519295491278172, 0.10472097992897034, 0.07220694422721863, 0.021218938753008842, -0.020897360518574715, -0.010840357281267643, 0.01603815145790577, 0.07231224328279495, 0.08439610153436661, -0.04080333188176155, -6.5480641353588e-08, -0.057805612683296204, -0.000762010517064482, 0.05617837607860565, 0.09877482056617737, 0.03158101066946983, -0.0017320213373750448, -0.02064240537583828, 0.07300186157226562, 0.0569017119705677, -0.033831119537353516, 0.07627519220113754, -0.04256899654865265, -0.059762824326753616, 0.0032591524068266153, 0.0636511817574501, -0.0016865733778104186, 0.00951545499265194, -0.03700989857316017, -0.06705588102340698, -0.04709320142865181, 0.05230216681957245, 0.008703183382749557, 0.0026592276990413666, -0.013899489305913448, 0.10023506730794907, -0.07098619639873505, -0.00658870954066515, 0.15066149830818176, 0.01614595390856266, -0.013191892765462399, 0.0021101003512740135, 0.018720021471381187, -0.034766700118780136, 0.047911807894706726, 0.010694201104342937, -0.04421950504183769, 0.04561113566160202, 0.004345445428043604, 0.05099482834339142, 0.016540128737688065, 0.0014361654175445437, 0.019786644726991653, -0.021002614870667458, 0.019680863246321678, 0.03794262930750847, -0.022045398131012917, -0.12378807365894318, -0.0630548745393753, -0.013450508937239647, 0.016319045796990395, 0.012693611904978752, -0.05458171293139458, 0.06699434667825699, 0.07796499133110046, 0.008586017414927483, 0.04530670493841171, 0.029789967462420464, 0.039290279150009155, 0.07645907998085022, 0.028911380097270012, 0.046892814338207245, -0.021822627633810043, -0.1057855561375618, -0.020319227129220963], platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256}, link='https://decodingml.substack.com/p/dml-7-steps-to-build-a-production?r=1ttoeh'),\n",
       " EmbedArticle(id=UUID('ef7cf184-7e32-4014-a296-0c69a3fb50d4'), content='As you can see, the secret ingredients are not the LLM but the amount of data the quality of data how you process the data for compute power the ability to scale the system 3 steps to write a Python script to train your LLMs at scale Image by the Author . My advice If you don t plan to become an ML researcher, shift your focus from the latest models to your data and infrastructure. . ğ—¡ğ—¼ğ˜ğ—² Integrating serverless services, such as Beam, makes the deployment of your training pipeline fast seamless, leaving you to focus only on the last piece of the puzzle your data. Check out Beam s docs to find out more. 2. 7 steps to chain your prompts to build a production ready financial assistant using LLMs. ğŸ³ ğ˜€ğ˜ğ—²ğ—½ğ˜€ on how to ğ—°ğ—µğ—®ğ—¶ğ—» your ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€ to build a production ready ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜ using ğ—Ÿğ—Ÿğ— ğ˜€ When building LLM applications, you frequently have to divide your application into multiple steps prompts, which are known as chaining prompts . Here are 7 standard steps when building a financial assistant using LLMs or any other assistant ğ—¦ğ˜ğ—²ğ—½ ğŸ­ Check if the user s question is safe using OpenAI s Moderation API If the user s query is safe, move to ğ—¦ğ˜ğ—²ğ—½ ğŸ® ğ—¦ğ˜ğ—²ğ—½ ğŸ® Query your proprietary data e.g., financial news to enrich the prompt with fresh data additional context. To do so, you have to use an LM to embed the user s input use the embedding to query your proprietary data stored in a vector DB ğ˜•ğ˜°ğ˜µğ˜¦ You must use the same LM model to embed the data that will be stored in the vector DB the user s question used to query the vector DB ğ—¦ğ˜ğ—²ğ—½ ğŸ¯ Build the prompt using a predefined template the user s question extracted financial news as context your conversation history as context ğ—¦ğ˜ğ—²ğ—½ ğŸ° Call the LLM ğ—¦ğ˜ğ—²ğ—½ ğŸ± Check if the assistant s answer is safe using the OpenAI s Moderation API. If the assistant s answer is safe, move to ğ—¦ğ˜ğ—²ğ—½ ğŸ± ğ—¦ğ˜ğ—²ğ—½ ğŸ² Use an LLM to check if the final answer is satisfactory.', embedding=[0.008164103142917156, -0.03655910864472389, -0.043119002133607864, 0.05563740432262421, -0.02113465406000614, -0.10368650406599045, -0.04081755504012108, 0.04637264087796211, -0.05859220400452614, -0.038458891212940216, -0.011665649712085724, -0.006266363430768251, 0.03300414979457855, -0.04628869518637657, 0.04694069176912308, 0.062378544360399246, 0.08912088721990585, -0.014360706321895123, -0.06357289850711823, -0.06728394329547882, 0.038975488394498825, -0.036880847066640854, 0.0568864680826664, 0.0014496977673843503, 0.0032831281423568726, -0.017841624096035957, 0.0616118423640728, 0.0020282838959246874, 0.022667622193694115, -0.04846082255244255, 0.04332130029797554, 0.032609712332487106, 0.01122740563005209, 0.04169400408864021, 0.037737905979156494, 0.03992413729429245, 0.040878333151340485, 0.0009607347892597318, -0.06344455480575562, -0.0270816832780838, 0.05732457712292671, -0.07100291550159454, -0.03700057789683342, -0.044954564422369, 0.01815231703221798, -0.044075336307287216, -0.0006388569017872214, -0.06785878539085388, -0.05389891192317009, 0.002874046564102173, -0.0668225884437561, -0.055268533527851105, -0.035953592509031296, 0.04902634397149086, -0.06078401952981949, 0.008151845075190067, 0.06852009147405624, -0.03043278679251671, -0.04579348489642143, -0.03971073776483536, -0.04054750129580498, -0.0348954051733017, -0.04210537299513817, 0.01972990855574608, 0.054271575063467026, 0.04424131289124489, -0.002587526338174939, 0.0870436429977417, 0.06448482722043991, -0.049057573080062866, 0.0239176694303751, -0.11853543668985367, -0.06806816905736923, 0.015926053747534752, -0.05678259953856468, -0.03436477109789848, 0.06160181760787964, 0.046645183116197586, 0.044592343270778656, -0.02341443859040737, -0.010641111060976982, 0.010906217619776726, -0.0441405288875103, 0.06656847149133682, -0.037868984043598175, 0.003228889312595129, -0.038100697100162506, 0.09831608086824417, 0.054523032158613205, 0.0037372042424976826, 0.08807362616062164, -0.011939146555960178, -0.05783402919769287, 0.010695181787014008, 0.07164832949638367, 0.05020107328891754, -0.052192844450473785, -0.14305976033210754, -0.004707799758762121, 0.028370777145028114, 0.0075791156850755215, 0.004654196556657553, 0.05840376764535904, 0.042655933648347855, -0.0830288678407669, -0.008188947103917599, -0.008726680651307106, 0.07214419543743134, 0.03623996302485466, -0.048603057861328125, 0.061502519994974136, 0.0418381467461586, 0.01581377349793911, -0.013541231863200665, 0.061536841094493866, -0.027451947331428528, -0.071740061044693, -0.03251085430383682, -0.0035151983611285686, 0.07997824251651764, 0.024454623460769653, 0.009668263606727123, -0.02195560745894909, -0.023289773613214493, 0.031055698171257973, -0.012791858054697514, -0.09765675663948059, 5.9845846360682875e-33, 0.04093220829963684, 0.06882762163877487, 0.02959189936518669, -0.013200609013438225, 0.05354547128081322, 0.07023586332798004, 0.09680045396089554, 0.060741160064935684, -0.06834524869918823, 0.06603363156318665, -0.03532164543867111, 0.12543848156929016, -0.01888318918645382, -0.009069395251572132, 0.006669003516435623, -0.0923599824309349, 0.026187896728515625, 0.060999803245067596, 0.07941451668739319, -0.00033976760460063815, 0.04498567804694176, -0.03840544447302818, 0.06151517108082771, -0.0368412509560585, 0.08273535966873169, 0.02573169581592083, 0.010289418511092663, 0.017281334847211838, -0.02179604396224022, 0.02973552979528904, -0.022842396050691605, -0.02509564906358719, 0.009146274998784065, -0.01595151610672474, 0.0007654866785742342, 0.0017273053526878357, -0.1323567032814026, -0.06465776264667511, 0.02420397475361824, -0.06041719391942024, -0.028259404003620148, 0.043909236788749695, 0.0681985467672348, -0.03133435919880867, -0.12743626534938812, 0.045796457678079605, -0.004143199883401394, -0.005402758251875639, 0.010775414295494556, 0.04223252832889557, 0.038336317986249924, -0.05622832104563713, -0.047081343829631805, -0.06663662940263748, 0.0021064437460154295, -0.00891401432454586, 0.007373043801635504, -0.07114265859127045, -0.0002767261175904423, -0.10104988515377045, -0.032994482666254044, -0.06261438876390457, -0.03474586829543114, 0.08775889128446579, 0.008717583492398262, 0.020667603239417076, -0.08668871223926544, 0.045496225357055664, 0.02883792482316494, -0.02475784532725811, -0.04877575859427452, 0.04564989358186722, -0.04988330975174904, 0.00077037513256073, 0.039264313876628876, -0.06641964614391327, 0.049143873155117035, 0.020453596487641335, 0.024315042421221733, 0.0421714261174202, 0.06015072762966156, 0.06567271798849106, -0.04911282658576965, 0.024353569373488426, 0.07555832713842392, 0.05352284014225006, 0.012891493737697601, 0.012241433374583721, -0.023365307599306107, 0.08103516697883606, -0.07653124630451202, -0.035410117357969284, 0.03439527377486229, 0.13604557514190674, -0.014230229891836643, -5.583701756845532e-33, -0.03128393366932869, 0.020664770156145096, 0.011376211419701576, 0.06105980649590492, 0.03529830649495125, -0.03587980568408966, -0.013810775242745876, 0.014989970251917839, 0.015367100015282631, 0.0031567784026265144, -0.06751414388418198, -0.04081564396619797, -0.017404481768608093, -0.043038371950387955, -0.03350129351019859, -0.03900904953479767, -0.016249055042862892, -0.03956686705350876, 0.036841657012701035, -0.011087564751505852, -0.04132387042045593, 0.0878136083483696, -0.09313960373401642, 0.02520582638680935, -0.046756044030189514, 0.01204200554639101, -0.036972880363464355, -0.004746585618704557, 0.009442140348255634, 0.05021228641271591, 0.002100536832585931, -0.024506306275725365, -0.006747216451913118, -0.00638531194999814, -0.10579794645309448, 0.014808562584221363, 0.06026832014322281, 0.04134400933980942, 0.045123789459466934, 0.024151941761374474, 0.10994348675012589, 0.011338556185364723, -0.04976954311132431, -0.027459673583507538, -0.06264270842075348, -0.053453490138053894, -0.029327018186450005, 0.02041190303862095, 0.030854467302560806, -0.05973876640200615, -0.028189830482006073, 0.021719658747315407, -0.03360142931342125, 0.0028490254189819098, -0.05977838486433029, -0.03559255599975586, 0.11271367222070694, -0.03443684056401253, -0.11113522201776505, -0.08560437709093094, -0.043713267892599106, -0.025780929252505302, 0.1020694226026535, 0.0001334376574959606, -0.04661019146442413, -0.010668418370187283, -0.01640453189611435, 0.04035855829715729, -0.0973588302731514, -0.0683511421084404, -0.023308666422963142, -0.08295346051454544, 0.05060191825032234, 0.009811638854444027, -0.016821764409542084, -0.027715710923075676, -0.08531138300895691, -0.11573471128940582, 0.05153180658817291, 0.0274064801633358, 0.004895300604403019, -0.00555867375805974, 0.005852843634784222, 0.125832200050354, 0.00039242577622644603, 0.0136795062571764, 0.13670693337917328, 0.01413826085627079, 0.033488839864730835, 0.006925384979695082, -0.07866775244474411, 0.05781179293990135, 0.022687025368213654, 0.04577268287539482, 0.056947577744722366, -6.31836911679784e-08, -0.0925590991973877, -0.07384584099054337, 0.009953279048204422, 0.04991522431373596, -0.013330895453691483, -0.036014262586832047, -0.03025304339826107, 0.12018073350191116, 0.05248329043388367, -0.03238734230399132, 0.0905839204788208, -0.03289411962032318, -0.1081748902797699, -0.004855995066463947, 0.052938006818294525, 0.08911911398172379, 0.05234385281801224, -0.019910158589482307, -0.05754346400499344, -0.054615188390016556, 0.030694495886564255, -0.025239886716008186, 0.0709746703505516, 0.017884287983179092, 0.053330108523368835, -0.05187070369720459, 0.01194780133664608, 0.10487490147352219, -0.010901963338255882, -6.569953256985173e-05, -0.031340040266513824, -0.0955614224076271, -0.011927413754165173, 0.07779769599437714, 0.005914090666919947, -0.010437200777232647, 0.045362439006567, -0.03653931990265846, 0.0494205467402935, -0.0048257820308208466, -0.0394369401037693, -0.0072042569518089294, -0.004334932658821344, 0.046222563832998276, 0.01639384962618351, -0.001904115080833435, -0.07355954498052597, -0.059204261749982834, -0.021288299933075905, 0.0019435463473200798, 0.04082276299595833, -0.0766775831580162, 0.041484538465738297, 0.02138018049299717, 0.06151478737592697, 0.03422534093260765, 0.02610933966934681, 0.018704425543546677, 0.005122265312820673, 0.07416065037250519, 0.06461508572101593, -0.02156178466975689, -0.039523646235466, -0.05931071937084198], platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256}, link='https://decodingml.substack.com/p/dml-7-steps-to-build-a-production?r=1ttoeh'),\n",
       " EmbedArticle(id=UUID('fc1a5c2a-8529-4633-9368-84c01b29b77d'), content='To do so, you build a prompt using the following a validation predefined template the user s initial question the assistants answer The LLM has to give a yes or no answer. Thus, if it answers yes, we show the final answer to the user. Otherwise, we will return a predefined response, such as Sorry, we couldn t answer your question because we don t have enough information. ğ—¦ğ˜ğ—²ğ—½ ğŸ³ Add the user s question and assistant s answer to a history cache. Which will be used to enrich the following prompts with the current conversation. Just to remind you, the assistant should support a conversation. Thus, it needs to know what happened in the previous questions. In practice, you usually keep only the latest N question, answer tuples or a conversation summary to keep your context length under control. 7 Steps to Build a Production Ready Financial Assistant Using LLMs Image by the Author If you want to see this strategy in action, check out our new FREE Hands on LLMs course work in progress give it a on GitHub to stay updated with its latest progress. Extra 3 key resources on how to monitor your ML models In the last month, I read 100 ML monitoring articles. I trimmed them for you to 3 key resources 1 . A series of excellent articles made by Arize AI that will make you understand what ML monitoring is all about. Arize Articles 2 . The Evidently AI Blog, where you can find answers to all your questions regarding ML monitoring. Evidently Blog 3 . The monitoring hands on examples hosted by DataTalksClub will teach you how to implement an ML monitoring system. DataTalks Course After wasting a lot of time reading other resources... Using these 3 resources is a solid start for learning about monitoring ML systems. That s it for today See you next Thursday at 9 00 a.m. CET. Have a fantastic weekend! Paul Whenever you re ready, here is how I can help you 1.', embedding=[-0.02353314310312271, 0.017998358234763145, -0.0002246299700345844, 0.0017966284649446607, -0.01660754159092903, 0.0056963711977005005, 0.039838362485170364, 0.03994060680270195, 0.006011479534208775, -0.05100559443235397, -0.07510862499475479, -0.06127997487783432, -0.0208891611546278, -0.052798084914684296, -0.018035393208265305, 0.020926233381032944, 0.10341494530439377, -0.04782577604055405, -0.06964338570833206, 0.008806928060948849, 0.01569315232336521, -0.007755452301353216, 0.05954333767294884, 0.04337992146611214, -0.028488799929618835, -0.0071121156215667725, 0.07944276183843613, -0.002238223794847727, -0.007123359013348818, -0.0110546899959445, 0.04798600822687149, -0.02418501302599907, 0.05350784212350845, 0.07711432874202728, 0.052956145256757736, 0.02088121324777603, 0.007090336177498102, 0.03918841481208801, -0.021640149876475334, -0.038635462522506714, -0.08843481540679932, -0.1025194302201271, -0.05276089906692505, -0.017660995945334435, 0.03259608522057533, -0.09558366239070892, -0.013019912876188755, -0.003128272946923971, -0.011081875301897526, 0.0266142375767231, -0.0030149572994560003, -0.06748150289058685, -0.003063046373426914, 0.027370760217308998, -0.02595299482345581, 0.04569101706147194, 0.011565708555281162, 0.033791594207286835, -0.09219250828027725, -0.0072146933525800705, -0.06434386968612671, -0.058057233691215515, -0.07181048393249512, 0.050240300595760345, 0.021600577980279922, 0.06719410419464111, -0.034270018339157104, 0.05496232211589813, 0.0035240259021520615, 0.03857044875621796, -0.07592061907052994, -0.02025444246828556, -0.04359724745154381, 0.03167618811130524, -0.06833969056606293, 0.0282250065356493, 0.006902642548084259, -0.0035090865567326546, 0.03943709656596184, -0.007992628030478954, -0.02414468489587307, 0.006200273055583239, 0.011994201689958572, 0.02592296339571476, 0.015984654426574707, 0.015600739046931267, 0.02693013846874237, 0.10808804631233215, 0.005695011001080275, -0.021152526140213013, 0.0446007139980793, -0.012841801159083843, -0.004088090732693672, 0.016212457790970802, 0.06162705272436142, 0.040457382798194885, -0.02309386618435383, -0.08624114096164703, 0.006915057078003883, 0.06844066083431244, 0.039574652910232544, 0.05018697306513786, 0.010231969878077507, -0.06487607955932617, -0.06043912470340729, 0.05704798921942711, -0.024792060256004333, 0.029198404401540756, 0.009758171625435352, -0.013686536811292171, -0.0007628210005350411, 0.046360090374946594, 0.016785310581326485, 0.004334707744419575, 0.021343201398849487, -0.0969860777258873, 0.04811934009194374, 0.010538153350353241, -0.05858496576547623, 0.005674054846167564, 0.08359132707118988, 0.006518951151520014, -0.01721658930182457, 0.01674816571176052, 0.09820859134197235, -0.06515282392501831, 0.033573050051927567, 3.560808589536827e-33, 0.1039198637008667, 0.04069611430168152, 0.061515409499406815, 0.10000544786453247, -0.012797964736819267, 0.12179114669561386, 0.005616641137748957, 0.07955268025398254, 0.02496342360973358, -0.01339638140052557, 0.048012372106313705, 0.03392740711569786, -0.018063344061374664, -0.0725899487733841, -0.054024022072553635, -0.008626335300505161, -0.004120680969208479, 0.06610230356454849, 0.006577521562576294, -0.028761249035596848, 0.02257644198834896, -0.07000003010034561, 0.051584843546152115, 0.003632877953350544, 0.13554607331752777, 0.014696910046041012, 0.05282597616314888, 0.01165600586682558, 0.028496982529759407, -0.014177081175148487, -0.02871561236679554, -0.08889365196228027, -0.06013656407594681, -0.024768734350800514, 0.0020336632151156664, 0.03171681612730026, -0.02527516894042492, -0.008709713816642761, 0.02636520192027092, -0.03138403967022896, 0.03606985881924629, -0.009685572236776352, 0.07141172140836716, 0.013967271894216537, -0.1293002814054489, -0.04153544828295708, 0.039120327681303024, -0.04246842488646507, 0.012589514255523682, -0.012136806733906269, 0.044000789523124695, -0.04906906187534332, -0.12759657204151154, -0.0666632130742073, -0.01634400710463524, -0.06078490614891052, -0.04249262437224388, -0.042216286063194275, -0.03320606052875519, -0.04446815326809883, 0.07164141535758972, -0.03527291864156723, -0.014760740101337433, 0.06864931434392929, -0.024439875036478043, -0.019568851217627525, -0.06339624524116516, 0.05913263186812401, 0.040941815823316574, -0.029940765351057053, -0.02910505421459675, -0.03474128991365433, 0.0035610953345894814, 0.006749601569026709, -0.03200862929224968, 0.0025248704478144646, -0.032175350934267044, 0.04614080488681793, 0.06027721241116524, 0.0067327567376196384, 0.10548397898674011, -0.03599587827920914, -0.033488113433122635, -0.023627907037734985, 0.07692711800336838, -0.02671319805085659, 0.11539049446582794, -0.035840995609760284, 0.01061249803751707, -0.02710118517279625, -0.06875570118427277, -0.04464157298207283, 0.02068699523806572, 0.08080419152975082, 0.03567522019147873, -3.882136215147838e-33, 0.059984590858221054, -0.03732971101999283, -0.06888626515865326, 0.030857931822538376, -0.001681127236224711, -0.011899039149284363, -0.02406558394432068, 0.10389139503240585, -0.04121001809835434, -0.044374119490385056, -0.02378816157579422, -0.025005551055073738, 0.0016451823757961392, 0.004814440850168467, -0.0373682826757431, -0.01577288843691349, 0.008347571827471256, -0.10366136580705643, 0.0008563128649257123, 0.00841853953897953, -0.06227223947644234, 0.024248454719781876, -0.07529555261135101, -0.04775708168745041, -0.05687059834599495, 0.015882374718785286, -0.000844416324980557, 0.020698964595794678, -0.02012999728322029, 0.00836947001516819, -0.029079709202051163, -0.05918673053383827, 0.023622725158929825, -0.0011217884020879865, -0.07613851130008698, -0.004711984656751156, 0.0707036480307579, -0.06011079624295235, 0.006882722023874521, 0.134877011179924, 0.1446496993303299, -0.03981158882379532, -0.027727657929062843, 0.011775374412536621, -0.07093797624111176, -0.098861463367939, -0.015584507957100868, -0.00871366634964943, -0.01841512881219387, 0.007381330709904432, 6.037393904989585e-05, -0.03945348039269447, -0.041497644037008286, -0.02537839487195015, -0.015099105425179005, 0.02248545177280903, 0.029669851064682007, -0.01121248584240675, 0.03180871531367302, -0.042870957404375076, -0.0710744708776474, 0.018050026148557663, 0.06099601462483406, -0.05714238062500954, 0.07333137840032578, -0.008103755302727222, -0.032941266894340515, -0.04714563488960266, -0.02989255264401436, -0.0511445477604866, 0.04077605903148651, 0.003991712350398302, -0.02813669666647911, -0.022977113723754883, 0.12489444017410278, -0.03605140745639801, -0.034799281507730484, -0.19996275007724762, 0.01841280795633793, -0.0914725661277771, -0.03161096200346947, 0.019985925406217575, 0.023729918524622917, 0.13137249648571014, -0.04447437822818756, 0.023044271394610405, 0.07084731757640839, 0.08571657538414001, 0.02396300435066223, 0.036729540675878525, 0.010839301161468029, -0.0278828963637352, 0.025612803176045418, 0.032843198627233505, -0.038597967475652695, -5.728457352915939e-08, -0.07654128968715668, 0.057614270597696304, 0.03728630393743515, 0.09260038286447525, 0.06380699574947357, -0.045915208756923676, -0.009509439580142498, 0.03722059726715088, 0.06035303696990013, -0.0761881172657013, 0.02714531682431698, -0.00857606716454029, -0.016498254612088203, -0.03802971541881561, 0.08253728598356247, 0.0335557647049427, 0.036501698195934296, -0.03378196060657501, -0.051748622208833694, -0.07165887951850891, 0.10412668436765671, 0.05450727045536041, -0.02057279273867607, 0.09778419137001038, 0.04758941009640694, 0.04238709434866905, -0.04818696528673172, 0.1490968018770218, -0.05140518769621849, -0.0028762146830558777, 0.019186455756425858, -0.028284389525651932, -0.05539880692958832, 0.014801896177232265, 0.012385524809360504, -0.025222178548574448, -0.0005982829607091844, -0.04968098923563957, 0.06354257464408875, -0.02276432327926159, -0.03461515158414841, -0.004310186952352524, -0.017348947003483772, 0.045353420078754425, 0.01946994662284851, 0.009101469069719315, -0.03491858392953873, -0.1261059194803238, -0.05990837886929512, -0.07425183057785034, -0.019005775451660156, -0.11755483597517014, 0.11239124834537506, 0.07381459325551987, -0.01013422291725874, -0.023448482155799866, 0.056118596345186234, 0.07728502154350281, 0.00026915036141872406, 0.032737571746110916, 0.038568709045648575, 0.022105904296040535, -0.08343351632356644, -0.03028968721628189], platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256}, link='https://decodingml.substack.com/p/dml-7-steps-to-build-a-production?r=1ttoeh'),\n",
       " EmbedArticle(id=UUID('5c3fdb99-76db-4f48-b352-628e8a250f61'), content='The Full Stack 7 Steps MLOps Framework a 7 lesson FREE course that will walk you step by step through how to design, implement, train, deploy, and monitor an ML batch system using MLOps good practices. It contains the source code 2.5 hours of reading video materials on Medium. 2. Machine Learning MLOps Blog in depth topics about designing and productionizing ML systems using MLOps. 3. Machine Learning MLOps Hub a place where all my work is aggregated in one place courses, articles, webinars, podcasts, etc. . 5 Share this post DML 7 steps to build a production ready financial assistant using LLMs decodingml.substack.com Copy link Facebook Email Note Other Share PreviousNext Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Note Other This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en', embedding=[-0.028146980330348015, -0.08340126276016235, -0.003504457650706172, 0.002426858525723219, 0.03537066653370857, -0.06719035655260086, -0.037356071174144745, -0.033196333795785904, -0.13233481347560883, 0.027124876156449318, -0.04302966967225075, -0.02872433140873909, -0.044978708028793335, -0.08420354872941971, -0.034908320754766464, 0.07010627537965775, 0.0823192372918129, -0.013078714720904827, -0.032107703387737274, -0.11198189854621887, -0.011725730262696743, -0.06282511353492737, 0.00833771750330925, 0.02252284623682499, 0.021072864532470703, 0.012938745319843292, 0.007238276768475771, 0.05544119328260422, 0.02462122216820717, -0.0542089082300663, 0.047555189579725266, -0.047638341784477234, 0.07449416071176529, 0.051118671894073486, 0.04904831945896149, 0.12100333720445633, 0.07975758612155914, -0.05882656201720238, -0.001044611562974751, 0.030946481972932816, -0.02565808594226837, -0.11203622817993164, -0.03969218581914902, -0.06755022704601288, 0.045418962836265564, -0.007059962954372168, -0.044816192239522934, -0.1555178165435791, -0.022422252222895622, -0.004154138267040253, -0.08452285081148148, -0.12780258059501648, -0.0049688988365232944, -0.032749731093645096, -0.10982268303632736, 0.0063181049190461636, 0.05546818673610687, 0.05629940703511238, -0.03797682374715805, -0.0493604838848114, -0.021963367238640785, -0.08647401630878448, -0.03494848683476448, 0.016970179975032806, 0.040087152272462845, 0.01789584383368492, -0.03668937832117081, 0.10641341656446457, 0.02817896008491516, -0.0324743427336216, -0.0842844769358635, 0.0017245764611288905, -0.02439829707145691, 0.07227714359760284, -0.061191633343696594, 0.006917810998857021, 0.04957203194499016, -0.062069110572338104, 0.07579414546489716, -0.04714646935462952, 0.02359013445675373, 0.09182138741016388, -0.049104295670986176, 0.03349441662430763, -0.04868078976869583, -0.034657884389162064, 0.004591377452015877, 0.043890029191970825, 0.049487728625535965, 0.0026331129483878613, -0.022269099950790405, -0.0018461159197613597, 0.012044195085763931, -0.020237529650330544, 0.01457500085234642, 0.045072462409734726, -0.011738680303096771, -0.06294609606266022, -0.014863151125609875, 0.053537968546152115, -0.04898891597986221, 0.060559336096048355, 0.04079018905758858, -0.06593862175941467, -0.07163811475038528, 0.008500234223902225, -0.02277950569987297, 0.10556595027446747, 0.07370232045650482, -0.08751744031906128, 0.012858246453106403, 0.1114199236035347, -0.034594446420669556, -0.0665062889456749, 0.05045374110341072, -0.030999919399619102, 0.05646095797419548, -0.0705137699842453, 0.028208917006850243, 0.07721887528896332, 0.00186357949860394, 0.0416078120470047, -0.012414592318236828, -0.059305887669324875, 0.011177610605955124, -0.06994523108005524, -0.06989064812660217, 1.4440342720261129e-32, 0.04822232946753502, 0.017167214304208755, 0.01662994548678398, 0.04422556608915329, 0.07683804631233215, -0.02559412457048893, 0.05054033547639847, -0.01171580795198679, -0.07917255163192749, 0.0377882681787014, 0.01735682785511017, 0.031255051493644714, -0.04369834065437317, 0.0025820306036621332, -0.0014712276170030236, -0.1414809674024582, 0.05373702570796013, 0.08311314135789871, 0.026025263592600822, -0.016680365428328514, 0.03186926618218422, -0.0861482322216034, 0.001487065339460969, -0.02302321419119835, 0.05388219282031059, 0.041781939566135406, 0.05352672562003136, 0.01476940419524908, 0.06414489448070526, 0.040634360164403915, -0.02880382351577282, 0.04485311731696129, -0.058214399963617325, 0.026981929317116737, 0.021408388391137123, -0.022329479455947876, -0.05246223881840706, -0.03944519907236099, 0.03396013751626015, -0.01880848966538906, -0.07637646049261093, -0.014799064956605434, 0.05024104192852974, -0.1045280173420906, -0.07059063762426376, 0.02268393523991108, 0.008136904798448086, -0.06914687901735306, 0.14395734667778015, 0.01258873101323843, 0.035253457725048065, -0.06140228733420372, 0.021687200292944908, -0.04004568234086037, 0.024745343253016472, 0.017751852050423622, -0.003431834978982806, -0.05170800909399986, -0.003092521335929632, 0.01113421842455864, -0.01585579290986061, -0.009075168520212173, 0.005152956582605839, 0.025610212236642838, 0.021789537742733955, -0.06683038920164108, 0.013746418058872223, -0.0015539192827418447, 0.04094500467181206, -0.010016394779086113, 0.018136873841285706, 0.05075050890445709, 0.0701979473233223, 0.024075878784060478, 0.005693621933460236, -0.004126125946640968, 0.0018613783176988363, 0.003885207697749138, -0.0380018875002861, 0.024832725524902344, 0.055269114673137665, 0.03384540602564812, -0.045432839542627335, -0.062127772718667984, 0.059697337448596954, 0.04178384691476822, -0.015774521976709366, -0.05591272562742233, 0.053510189056396484, 0.016813186928629875, -0.08464397490024567, -0.0809222012758255, 0.042568791657686234, -0.012250018306076527, -0.017173534259200096, -1.2775778067307474e-32, 0.05373113602399826, 0.05513187497854233, -0.1023421436548233, 0.0855766087770462, 0.0007215070072561502, 0.0005377756315283477, -0.0074901157058775425, 0.03261169418692589, -0.04006144404411316, 0.06220633164048195, -0.02374681457877159, -0.040876951068639755, -0.06761544197797775, 0.016854913905262947, -0.05517012998461723, -0.061120759695768356, -0.0005554333911277354, -0.100822813808918, 0.05661802738904953, 0.03335355222225189, -0.045410044491291046, 0.08766426146030426, -0.07164232432842255, 0.037931639701128006, 0.037692874670028687, 0.012032011523842812, -0.09734197705984116, 0.11606694757938385, 0.021282874047756195, 0.0009458536515012383, 0.0009903514292091131, -0.014086317270994186, 0.024639155715703964, 0.019259179010987282, -0.006335053127259016, -0.039662253111600876, 0.02368989586830139, 0.014818212948739529, 0.06035027652978897, 0.00956109818071127, 0.05587051436305046, -0.047373149544000626, -0.08706074208021164, -0.05543997511267662, -0.036904264241456985, -0.04675905779004097, -0.018198583275079727, 0.0204121395945549, 0.05031128600239754, -0.06473421305418015, -0.07788872718811035, -0.02014678157866001, -0.03131415322422981, -0.0348496250808239, -0.04028993099927902, -0.00046546239173039794, 0.017810828983783722, 0.01246440876275301, -0.05474527180194855, 0.04325363412499428, -0.06525231897830963, -0.008000647649168968, 0.07168009132146835, 0.01030823215842247, 0.06891793012619019, -0.002033885335549712, -0.06115085631608963, 0.04679366201162338, -0.14335553348064423, -0.07399120181798935, -0.010272286832332611, -0.013823113404214382, 0.015165354125201702, -0.02570021152496338, -0.036355651915073395, -0.03593289852142334, -0.0029649382922798395, -0.055662088096141815, -0.005217237398028374, 0.056394610553979874, 0.01845104992389679, -0.02660287171602249, -0.009816983714699745, 0.10876185446977615, 0.03051024302840233, 0.04224967584013939, 0.05775712430477142, -0.009958148002624512, -0.0256354957818985, -0.04128507897257805, -0.06867394596338272, 0.04287872463464737, 0.05915704742074013, 0.10627712309360504, 0.025188425555825233, -6.889570869361705e-08, -0.08180157095193863, 0.0196344293653965, -0.006237979047000408, 0.02052254229784012, 0.01958637312054634, 0.06293769925832748, 0.006084583234041929, 0.051261208951473236, 0.009995845146477222, 0.01348115038126707, 0.010491019114851952, -0.06015323847532272, -0.031288377940654755, 0.05563237890601158, 0.046683382242918015, 0.03630383685231209, -0.02134750969707966, 0.06616801768541336, -0.039923045784235, 0.0012287520803511143, 0.08824408799409866, -0.04108114913105965, 0.026102298870682716, 0.0032809863332659006, 0.03334721922874451, -0.042906519025564194, 0.004118337295949459, 0.09490733593702316, 0.04925425350666046, 0.016560552641749382, -0.0041815899312496185, 0.03985106572508812, 0.008099626749753952, -0.003887082217261195, 0.03667721897363663, 0.048600129783153534, 0.0063570658676326275, -0.018816592171788216, 0.03587506711483002, -0.016163496300578117, -0.0133873475715518, 0.022819310426712036, 0.01752128079533577, 0.00825442187488079, -0.008161383680999279, 0.05986227095127106, -0.08387625962495804, -0.07207009196281433, -0.0050116488710045815, 0.021595770493149757, 0.001695522223599255, -0.029186883941292763, 0.06339788436889648, 0.09916801750659943, 0.10164037346839905, 0.04376828297972679, 0.0344996303319931, -0.010357129387557507, 0.11048659682273865, 0.1146082803606987, 0.03718164190649986, -0.016162613406777382, -0.024668371304869652, -0.015460735186934471], platform='decodingml.substack.com', author_id=UUID('b5fa1f08-75f0-402d-8e88-d1357e346d9e'), author_full_name='Paul Iusztin', metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256}, link='https://decodingml.substack.com/p/dml-7-steps-to-build-a-production?r=1ttoeh')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmtwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
