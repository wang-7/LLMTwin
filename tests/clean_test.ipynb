{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbd6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# query one document\n",
    "from llmtwin.domain.document import ArticleDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4cf4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': UUID('c5de10b1-7cd2-4625-9a17-3359fba04afa'), 'content': {'Title': 'Maxime Labonne - Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth', 'Subtitle': None, 'Content': 'Maxime Labonne\\n\\n  * __LLM Course\\n  * __Hands-On GNNs\\n  * __Research\\n  * __About\\n\\n  * __\\n  * __\\n  * __\\n  * \\n\\n__\\n\\n  1. üîß **LLM Post-training**\\n  2. Fine-tune Llama 3.1 8B\\n\\n  1. üîß **LLM Post-training**\\n  2. Fine-tune Llama 3.1 8B\\n\\n# Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth\\n\\nA beginner‚Äôs guide to state-of-the-art supervised fine-tuning\\n\\nLarge Language Models\\n\\nAuthor\\n\\nMaxime Lbonne\\n\\nPublished\\n\\nJuly 29, 2024\\n\\n  * üîß **LLM Post-training** __\\n\\n    * Fine-tune Llama 2 in Colab\\n\\n    * Fine-tune Llama 2 in Axolotl\\n\\n    * Fine-tune Mistral-7b with DPO\\n\\n    * Fine-tune Llama 3 with ORPO\\n\\n    * Fine-tune Llama 3.1 8B\\n\\n    * Merge LLMs with mergekit\\n\\n    * Create Mixture of Experts\\n\\n    * Uncensor any LLM\\n\\n  * * * *\\n\\n  * ‚ö° **LLM Quantization** __\\n\\n    * Intro to Quantization\\n\\n    * Quantization with GPTQ\\n\\n    * Quantization with GGML\\n\\n    * Quantization with ExLlamaV2\\n\\n  * * * *\\n\\n  * üó£Ô∏è **LLM stuff** __\\n\\n    * ChatGPT + KG\\n\\n    * Decoding Strategies\\n\\n    * Agentic data generation\\n\\n  * * * *\\n\\n  * üåê **Graph neural networks** __\\n\\n    * Graph Convolution Network\\n\\n    * Graph Attention Network\\n\\n    * GraphSAGE\\n\\n    * Graph Isomorphism Network\\n\\n  * * * *\\n\\n  * ü•á **Linear programming** __\\n\\n    * Linear Programming\\n\\n    * Integer Programming\\n\\n    * Constraint Programming\\n\\n    * Nonlinear Programming\\n\\n  * * * *\\n\\n  * üåÄ **Miscellaneous** __\\n\\n    * Q-learning\\n\\n    * Minecraft Bot\\n\\n    * Loops in Pandas\\n\\n    * What is a Tensor\\n\\n## **Sections**\\n\\n  * üîß Supervised Fine-Tuning\\n  * ‚öñÔ∏è SFT Techniques\\n  * ü¶ô Fine-Tune Llama 3.1 8B\\n  * Conclusion\\n\\nPre-order the **LLM Engineer‚Äôs Handbook**, my new book to master the art of\\nLLMs from concept to productionüëá\\n\\nThe recent release of Llama 3.1 offers models with an incredible level of\\nperformance, closing the gap between closed-source and open-weight models.\\nInstead of using frozen, general-purpose LLMs like GPT-4o and Claude 3.5, you\\ncan fine-tune Llama 3.1 for your specific use cases to achieve better\\nperformance and customizability at a lower cost.\\n\\nIn this article, we will provide a comprehensive overview of supervised fine-\\ntuning. We will compare it to prompt engineering to understand when it makes\\nsense to use it, detail the main techniques with their pros and cons, and\\nintroduce major concepts, such as LoRA hyperparameters, storage formats, and\\nchat templates. Finally, we will implement it in practice by fine-tuning Llama\\n3.1 8B in Google Colab with state-of-the-art optimization using Unsloth.\\n\\nAll the code used in this article is available on Google Colab and in the LLM\\nCourse. Special thanks to Daniel Han for answering my questions.\\n\\n## üîß Supervised Fine-Tuning\\n\\nSupervised Fine-Tuning (SFT) is a method to **improve and customize** pre-\\ntrained LLMs. It involves retraining base models on a smaller dataset of\\ninstructions and answers. The main goal is to transform a basic model that\\npredicts text into an assistant that can follow instructions and answer\\nquestions. SFT can also enhance the model‚Äôs overall performance, add new\\nknowledge, or adapt it to specific tasks and domains. Fine-tuned models can\\nthen go through an optional preference alignment stage (see my article about\\nDPO) to remove unwanted responses, modify their style, and more.\\n\\nThe following figure shows an instruction sample. It includes a system prompt\\nto steer the model, a user prompt to provide a task, and the output the model\\nis expected to generate. You can find a list of high-quality open-source\\ninstruction datasets in the üíæ LLM Datasets GitHub repo.\\n\\nBefore considering SFT, I recommend trying prompt engineering techniques like\\n**few-shot prompting** or **retrieval augmented generation** (RAG). In\\npractice, these methods can solve many problems without the need for fine-\\ntuning, using either closed-source or open-weight models (e.g., Llama 3.1\\nInstruct). If this approach doesn‚Äôt meet your objectives (in terms of quality,\\ncost, latency, etc.), then SFT becomes a viable option when instruction data\\nis available. Note that SFT also offers benefits like additional control and\\ncustomizability to create personalized LLMs.\\n\\nHowever, SFT has limitations. It works best when leveraging knowledge already\\npresent in the base model. Learning completely new information like an unknown\\nlanguage can be challenging and lead to more frequent hallucinations. For new\\ndomains unknown to the base model, it is recommended to continuously pre-train\\nit on a raw dataset first.\\n\\nOn the opposite end of the spectrum, instruct models (i.e., already fine-tuned\\nmodels) can already be very close to your requirements. For example, a model\\nmight perform very well but state that it was trained by OpenAI or Meta\\ninstead of you. In this case, you might want to slightly steer the instruct\\nmodel‚Äôs behavior using preference alignment. By providing chosen and rejected\\nsamples for a small set of instructions (between 100 and 1000 samples), you\\ncan force the LLM to say that you trained it instead of OpenAI.\\n\\n## ‚öñÔ∏è SFT Techniques\\n\\nThe three most popular SFT techniques are full fine-tuning, LoRA, and QLoRA.\\n\\n**Full fine-tuning** is the most straightforward SFT technique. It involves\\nretraining all parameters of a pre-trained model on an instruction dataset.\\nThis method often provides the best results but requires significant\\ncomputational resources (several high-end GPUs are required to fine-tune a 8B\\nmodel). Because it modifies the entire model, it is also the most destructive\\nmethod and can lead to the catastrophic forgetting of previous skills and\\nknowledge.\\n\\n**Low-Rank Adaptation (LoRA)** is a popular parameter-efficient fine-tuning\\ntechnique. Instead of retraining the entire model, it freezes the weights and\\nintroduces small adapters (low-rank matrices) at each targeted layer. This\\nallows LoRA to train a number of parameters that is drastically lower than\\nfull fine-tuning (less than 1%), reducing both memory usage and training time.\\nThis method is non-destructive since the original parameters are frozen, and\\nadapters can then be switched or combined at will.\\n\\n**QLoRA (Quantization-aware Low-Rank Adaptation)** is an extension of LoRA\\nthat offers even greater memory savings. It provides up to 33% additional\\nmemory reduction compared to standard LoRA, making it particularly useful when\\nGPU memory is constrained. This increased efficiency comes at the cost of\\nlonger training times, with QLoRA typically taking about 39% more time to\\ntrain than regular LoRA.\\n\\nWhile QLoRA requires more training time, its substantial memory savings can\\nmake it the only viable option in scenarios where GPU memory is limited. For\\nthis reason, this is the technique we will use in the next section to fine-\\ntune a Llama 3.1 8B model on Google Colab.\\n\\n## ü¶ô Fine-Tune Llama 3.1 8B\\n\\nTo efficiently fine-tune a Llama 3.1 8B model, we‚Äôll use the Unsloth library\\nby Daniel and Michael Han. Thanks to its custom kernels, Unsloth provides 2x\\nfaster training and 60% memory use compared to other options, making it ideal\\nin a constrained environment like Colab. Unfortunately, Unsloth only supports\\nsingle-GPU settings at the moment. For multi-GPU settings, I recommend popular\\nalternatives like TRL and Axolotl (both also include Unsloth as a backend).\\n\\nIn this example, we will QLoRA fine-tune it on the mlabonne/FineTome-100k\\ndataset. It‚Äôs a subset of arcee-ai/The-Tome (without arcee-\\nai/qwen2-72b-magpie-en) that I re-filtered using HuggingFaceFW/fineweb-edu-\\nclassifier. Note that this classifier wasn‚Äôt designed for instruction data\\nquality evaluation, but we can use it as a rough proxy. The resulting FineTome\\nis an ultra-high quality dataset that includes conversations, reasoning\\nproblems, function calling, and more.\\n\\nLet‚Äôs start by installing all the required libraries.\\n\\n    \\n    \\n    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n    !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes __\\n\\nOnce installed, we can import them as follows.\\n\\n    \\n    \\n    import torch\\n    from trl import SFTTrainer\\n    from datasets import load_dataset\\n    from transformers import TrainingArguments, TextStreamer\\n    from unsloth.chat_templates import get_chat_template\\n    from unsloth import FastLanguageModel, is_bfloat16_supported __\\n\\nLet‚Äôs now load the model. Since we want to use QLoRA, I chose the pre-\\nquantized unsloth/Meta-Llama-3.1-8B-bnb-4bit. This 4-bit precision version of\\nmeta-llama/Meta-Llama-3.1-8B is significantly smaller (5.4 GB) and faster to\\ndownload compared to the original 16-bit precision model (16 GB). We load in\\nNF4 format using the bitsandbytes library.\\n\\nWhen loading the model, we must specify a maximum sequence length, which\\nrestricts its context window. Llama 3.1 supports up to 128k context length,\\nbut we will set it to 2,048 in this example since it consumes more compute and\\nVRAM. Finally, the `dtype` parameter automatically detects if your GPU\\nsupports the BF16 format for more stability during training (this feature is\\nrestricted to Ampere and more recent GPUs).\\n\\n    \\n    \\n    max_seq_length = 2048\\n    model, tokenizer = FastLanguageModel.from_pretrained(\\n        model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\\n        max_seq_length=max_seq_length,\\n        load_in_4bit=True,\\n        dtype=None,\\n    )__\\n\\nNow that our model is loaded in 4-bit precision, we want to prepare it for\\nparameter-efficient fine-tuning with LoRA adapters. LoRA has three important\\nparameters:\\n\\n  * **Rank** (r), which determines LoRA matrix size. Rank typically starts at 8 but can go up to 256. Higher ranks can store more information but increase the computational and memory cost of LoRA. We set it to 16 here.\\n  * **Alpha** (Œ±), a scaling factor for updates. Alpha directly impacts the adapters‚Äô contribution and is often set to 1x or 2x the rank value.\\n  * **Target modules** : LoRA can be applied to various model components, including attention mechanisms (Q, K, V matrices), output projections, feed-forward blocks, and linear output layers. While initially focused on attention mechanisms, extending LoRA to other components has shown benefits. However, adapting more modules increases the number of trainable parameters and memory needs.\\n\\nHere, we set r=16, Œ±=16, and target every linear module to maximize quality.\\nWe don‚Äôt use dropout and biases for faster training.\\n\\nIn addition, we will use Rank-Stabilized LoRA (rsLoRA), which modifies the\\nscaling factor of LoRA adapters to be proportional to 1/‚àör instead of 1/r.\\nThis stabilizes learning (especially for higher adapter ranks) and allows for\\nimproved fine-tuning performance as rank increases. Gradient checkpointing is\\nhandled by Unsloth to offload input and output embeddings to disk and save\\nVRAM.\\n\\n    \\n    \\n    model = FastLanguageModel.get_peft_model(\\n        model,\\n        r=16,\\n        lora_alpha=16,\\n        lora_dropout=0,\\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], \\n        use_rslora=True,\\n        use_gradient_checkpointing=\"unsloth\"\\n    )__\\n\\nWith this LoRA configuration, we‚Äôll only train 42 million out of 8 billion\\nparameters (0.5196%). This shows how much more efficient LoRA is compared to\\nfull fine-tuning.\\n\\nLet‚Äôs now load and prepare our dataset. Instruction datasets are stored in a\\n**particular format** : it can be Alpaca, ShareGPT, OpenAI, etc. First, we\\nwant to parse this format to retrieve our instructions and answers. Our\\nmlabonne/FineTome-100k dataset uses the ShareGPT format with a unique\\n‚Äúconversations‚Äù column containing messages in JSONL. Unlike simpler formats\\nlike Alpaca, ShareGPT is ideal for storing multi-turn conversations, which is\\ncloser to how users interact with LLMs.\\n\\nOnce our instruction-answer pairs are parsed, we want to reformat them to\\nfollow a **chat template**. Chat templates are a way to structure\\nconversations between users and models. They typically include special tokens\\nto identify the beginning and the end of a message, who‚Äôs speaking, etc. Base\\nmodels don‚Äôt have chat templates so we can choose any: ChatML, Llama3,\\nMistral, etc. In the open-source community, the ChatML template (originally\\nfrom OpenAI) is a popular option. It simply adds two special tokens\\n(`<|im_start|>` and `<|im_end|>`) to indicate who‚Äôs speaking.\\n\\nIf we apply this template to the previous instruction sample, here‚Äôs what we\\nget:\\n\\n    \\n    \\n    <|im_start|>system\\n    You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.<|im_end|>\\n    <|im_start|>user\\n    Remove the spaces from the following sentence: It prevents users to suspect that there are some hidden products installed on theirs device.\\n    <|im_end|>\\n    <|im_start|>assistant\\n    Itpreventsuserstosuspectthattherearesomehiddenproductsinstalledontheirsdevice.<|im_end|>\\n\\nIn the following code block, we parse our ShareGPT dataset with the `mapping`\\nparameter and include the ChatML template. We then load and process the entire\\ndataset to apply the chat template to every conversation.\\n\\n    \\n    \\n    tokenizer = get_chat_template(\\n        tokenizer,\\n        mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\\n        chat_template=\"chatml\",\\n    )\\n    \\n    def apply_template(examples):\\n        messages = examples[\"conversations\"]\\n        text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\\n        return {\"text\": text}\\n    \\n    dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\\n    dataset = dataset.map(apply_template, batched=True)__\\n\\nWe‚Äôre now ready to specify the training parameters for our run. I want to\\nbriefly introduce the most important hyperparameters:\\n\\n  * **Learning rate** : It controls how strongly the model updates its parameters. Too low, and training will be slow and may get stuck in local minima. Too high, and training may become unstable or diverge, which degrades performance.\\n  * **LR scheduler** : It adjusts the learning rate (LR) during training, starting with a higher LR for rapid initial progress and then decreasing it in later stages. Linear and cosine schedulers are the two most common options.\\n  * **Batch size** : Number of samples processed before the weights are updated. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, but they also require more memory. Gradient accumulation allows for effectively larger batch sizes by accumulating gradients over multiple forward/backward passes before updating the model.\\n  * **Num epochs** : The number of complete passes through the training dataset. More epochs allow the model to see the data more times, potentially leading to better performance. However, too many epochs can cause overfitting.\\n  * **Optimizer** : Algorithm used to adjust the parameters of a model to minimize the loss function. In practice, AdamW 8-bit is strongly recommended: it performs as well as the 32-bit version while using less GPU memory. The paged version of AdamW is only interesting in distributed settings.\\n  * **Weight decay** : A regularization technique that adds a penalty for large weights to the loss function. It helps prevent overfitting by encouraging the model to learn simpler, more generalizable features. However, too much weight decay can impede learning.\\n  * **Warmup steps** : A period at the beginning of training where the learning rate is gradually increased from a small value to the initial learning rate. Warmup can help stabilize early training, especially with large learning rates or batch sizes, by allowing the model to adjust to the data distribution before making large updates.\\n  * **Packing** : Batches have a pre-defined sequence length. Instead of assigning one batch per sample, we can combine multiple small samples in one batch, increasing efficiency.\\n\\nI trained the model on the entire dataset (100k samples) using an A100 GPU (40\\nGB of VRAM) on Google Colab. The training took 4 hours and 45 minutes. Of\\ncourse, you can use smaller GPUs with less VRAM and a smaller batch size, but\\nthey‚Äôre not nearly as fast. For example, it takes roughly 19 hours and 40\\nminutes on an L4 and a whopping 47 hours on a free T4.\\n\\nIn this case, I recommend only loading a subset of the dataset to speed up\\ntraining. You can do it by modifying the previous code block, like `dataset =\\nload_dataset(\"mlabonne/FineTome-100k\", split=\"train[:10000]\")` to only load\\n10k samples. Alternatively, you can use cheaper cloud GPU providers like\\nPaperspace, RunPod, or Lambda Labs.\\n\\n    \\n    \\n    trainer=SFTTrainer(\\n        model=model,\\n        tokenizer=tokenizer,\\n        train_dataset=dataset,\\n        dataset_text_field=\"text\",\\n        max_seq_length=max_seq_length,\\n        dataset_num_proc=2,\\n        packing=True,\\n        args=TrainingArguments(\\n            learning_rate=3e-4,\\n            lr_scheduler_type=\"linear\",\\n            per_device_train_batch_size=8,\\n            gradient_accumulation_steps=2,\\n            num_train_epochs=1,\\n            fp16=not is_bfloat16_supported(),\\n            bf16=is_bfloat16_supported(),\\n            logging_steps=1,\\n            optim=\"adamw_8bit\",\\n            weight_decay=0.01,\\n            warmup_steps=10,\\n            output_dir=\"output\",\\n            seed=0,\\n        ),\\n    )\\n    \\n    trainer.train()__\\n\\nNow that the model is trained, let‚Äôs test it with a simple prompt. This is not\\na rigorous evaluation but just a quick check to detect potential issues. We\\nuse `FastLanguageModel.for_inference()` to get 2x faster inference.\\n\\n    \\n    \\n    model = FastLanguageModel.for_inference(model)\\n    \\n    messages = [\\n        {\"from\": \"human\", \"value\": \"Is 9.11 larger than 9.9?\"},\\n    ]\\n    inputs = tokenizer.apply_chat_template(\\n        messages,\\n        tokenize=True,\\n        add_generation_prompt=True,\\n        return_tensors=\"pt\",\\n    ).to(\"cuda\")\\n    \\n    text_streamer = TextStreamer(tokenizer)\\n    _ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)__\\n\\nThe model‚Äôs response is ‚Äú9.9‚Äù, which is correct!\\n\\nLet‚Äôs now save our trained model. If you remember the part about LoRA and\\nQLoRA, what we trained is not the model itself but a set of adapters. There\\nare three save methods in Unsloth: `lora` to only save the adapters, and\\n`merged_16bit`/`merged_4bit` to merge the adapters with the model in 16-bit/\\n4-bit precision.\\n\\nIn the following, we merge them in 16-bit precision to maximize the quality.\\nWe first save it locally in the ‚Äúmodel‚Äù directory and then upload it to the\\nHugging Face Hub. You can find the trained model on mlabonne/FineLlama-3.1-8B.\\n\\n    \\n    \\n    model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\\n    model.push_to_hub_merged(\"mlabonne/FineLlama-3.1-8B\", tokenizer, save_method=\"merged_16bit\")__\\n\\nUnsloth also allows you to directly convert your model into GGUF format. This\\nis a quantization format created for llama.cpp and compatible with most\\ninference engines, like LM Studio, Ollama, and oobabooga‚Äôs text-generation-\\nwebui. Since you can specify different precisions (see my article about GGUF\\nand llama.cpp), we‚Äôll loop over a list to quantize it in `q2_k`, `q3_k_m`,\\n`q4_k_m`, `q5_k_m`, `q6_k`, `q8_0` and upload these quants on Hugging Face.\\nThe mlabonne/FineLlama-3.1-8B-GGUF contains all our GGUFs.\\n\\n    \\n    \\n    quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\\n    for quant in quant_methods:\\n        model.push_to_hub_gguf(\"mlabonne/FineLlama-3.1-8B-GGUF\", tokenizer, quant)__\\n\\nCongratulations, we fine-tuned a model from scratch and uploaded quants you\\ncan now use in your favorite inference engine. Feel free to try the final\\nmodel available on mlabonne/FineLlama-3.1-8B-GGUF. What to do now? Here are\\nsome ideas on how to use your model:\\n\\n  * **Evaluate** it on the Open LLM Leaderboard (you can submit it for free) or using other evals like in LLM AutoEval.\\n  * **Align** it with Direct Preference Optimization using a preference dataset like mlabonne/orpo-dpo-mix-40k to boost performance.\\n  * **Quantize** it in other formats like EXL2, AWQ, GPTQ, or HQQ for faster inference or lower precision using AutoQuant.\\n  * **Deploy** it on a Hugging Face Space with ZeroChat for models that have been sufficiently trained to follow a chat template (~20k samples).\\n\\n## Conclusion\\n\\nThis article provided a comprehensive overview of supervised fine-tuning and\\nhow to apply it in practice to a Llama 3.1 8B model. By leveraging QLoRA‚Äôs\\nefficient memory usage, we managed to fine-tune an 8B LLM on a super high-\\nquality dataset with limited GPU resources. We also provided more efficient\\nalternatives for bigger runs and suggestions for further steps, including\\nevaluation, preference alignment, quantization, and deployment.\\n\\nI hope this guide was useful. If you‚Äôre interested in learning more about\\nLLMs, I recommend checking the LLM Course. If you enjoyed this article, follow\\nme on X @maximelabonne and on Hugging Face @mlabonne. Good luck fine-tuning\\nmodels!\\n\\n__Copyright 2023, Maxime Labonne\\n\\n', 'language': 'en'}, 'platform': 'mlabonne.github.io', 'author_id': UUID('eff74089-0271-4319-8543-745c087f4f61'), 'author_full_name': 'Maxime Labonne', 'link': 'https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html'}\n"
     ]
    }
   ],
   "source": [
    "result = ArticleDocument.find_one({}, by_alias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "189727da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=UUID('c5de10b1-7cd2-4625-9a17-3359fba04afa') content={'Title': 'Maxime Labonne - Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth', 'Subtitle': None, 'Content': 'Maxime Labonne\\n\\n  * __LLM Course\\n  * __Hands-On GNNs\\n  * __Research\\n  * __About\\n\\n  * __\\n  * __\\n  * __\\n  * \\n\\n__\\n\\n  1. üîß **LLM Post-training**\\n  2. Fine-tune Llama 3.1 8B\\n\\n  1. üîß **LLM Post-training**\\n  2. Fine-tune Llama 3.1 8B\\n\\n# Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth\\n\\nA beginner‚Äôs guide to state-of-the-art supervised fine-tuning\\n\\nLarge Language Models\\n\\nAuthor\\n\\nMaxime Lbonne\\n\\nPublished\\n\\nJuly 29, 2024\\n\\n  * üîß **LLM Post-training** __\\n\\n    * Fine-tune Llama 2 in Colab\\n\\n    * Fine-tune Llama 2 in Axolotl\\n\\n    * Fine-tune Mistral-7b with DPO\\n\\n    * Fine-tune Llama 3 with ORPO\\n\\n    * Fine-tune Llama 3.1 8B\\n\\n    * Merge LLMs with mergekit\\n\\n    * Create Mixture of Experts\\n\\n    * Uncensor any LLM\\n\\n  * * * *\\n\\n  * ‚ö° **LLM Quantization** __\\n\\n    * Intro to Quantization\\n\\n    * Quantization with GPTQ\\n\\n    * Quantization with GGML\\n\\n    * Quantization with ExLlamaV2\\n\\n  * * * *\\n\\n  * üó£Ô∏è **LLM stuff** __\\n\\n    * ChatGPT + KG\\n\\n    * Decoding Strategies\\n\\n    * Agentic data generation\\n\\n  * * * *\\n\\n  * üåê **Graph neural networks** __\\n\\n    * Graph Convolution Network\\n\\n    * Graph Attention Network\\n\\n    * GraphSAGE\\n\\n    * Graph Isomorphism Network\\n\\n  * * * *\\n\\n  * ü•á **Linear programming** __\\n\\n    * Linear Programming\\n\\n    * Integer Programming\\n\\n    * Constraint Programming\\n\\n    * Nonlinear Programming\\n\\n  * * * *\\n\\n  * üåÄ **Miscellaneous** __\\n\\n    * Q-learning\\n\\n    * Minecraft Bot\\n\\n    * Loops in Pandas\\n\\n    * What is a Tensor\\n\\n## **Sections**\\n\\n  * üîß Supervised Fine-Tuning\\n  * ‚öñÔ∏è SFT Techniques\\n  * ü¶ô Fine-Tune Llama 3.1 8B\\n  * Conclusion\\n\\nPre-order the **LLM Engineer‚Äôs Handbook**, my new book to master the art of\\nLLMs from concept to productionüëá\\n\\nThe recent release of Llama 3.1 offers models with an incredible level of\\nperformance, closing the gap between closed-source and open-weight models.\\nInstead of using frozen, general-purpose LLMs like GPT-4o and Claude 3.5, you\\ncan fine-tune Llama 3.1 for your specific use cases to achieve better\\nperformance and customizability at a lower cost.\\n\\nIn this article, we will provide a comprehensive overview of supervised fine-\\ntuning. We will compare it to prompt engineering to understand when it makes\\nsense to use it, detail the main techniques with their pros and cons, and\\nintroduce major concepts, such as LoRA hyperparameters, storage formats, and\\nchat templates. Finally, we will implement it in practice by fine-tuning Llama\\n3.1 8B in Google Colab with state-of-the-art optimization using Unsloth.\\n\\nAll the code used in this article is available on Google Colab and in the LLM\\nCourse. Special thanks to Daniel Han for answering my questions.\\n\\n## üîß Supervised Fine-Tuning\\n\\nSupervised Fine-Tuning (SFT) is a method to **improve and customize** pre-\\ntrained LLMs. It involves retraining base models on a smaller dataset of\\ninstructions and answers. The main goal is to transform a basic model that\\npredicts text into an assistant that can follow instructions and answer\\nquestions. SFT can also enhance the model‚Äôs overall performance, add new\\nknowledge, or adapt it to specific tasks and domains. Fine-tuned models can\\nthen go through an optional preference alignment stage (see my article about\\nDPO) to remove unwanted responses, modify their style, and more.\\n\\nThe following figure shows an instruction sample. It includes a system prompt\\nto steer the model, a user prompt to provide a task, and the output the model\\nis expected to generate. You can find a list of high-quality open-source\\ninstruction datasets in the üíæ LLM Datasets GitHub repo.\\n\\nBefore considering SFT, I recommend trying prompt engineering techniques like\\n**few-shot prompting** or **retrieval augmented generation** (RAG). In\\npractice, these methods can solve many problems without the need for fine-\\ntuning, using either closed-source or open-weight models (e.g., Llama 3.1\\nInstruct). If this approach doesn‚Äôt meet your objectives (in terms of quality,\\ncost, latency, etc.), then SFT becomes a viable option when instruction data\\nis available. Note that SFT also offers benefits like additional control and\\ncustomizability to create personalized LLMs.\\n\\nHowever, SFT has limitations. It works best when leveraging knowledge already\\npresent in the base model. Learning completely new information like an unknown\\nlanguage can be challenging and lead to more frequent hallucinations. For new\\ndomains unknown to the base model, it is recommended to continuously pre-train\\nit on a raw dataset first.\\n\\nOn the opposite end of the spectrum, instruct models (i.e., already fine-tuned\\nmodels) can already be very close to your requirements. For example, a model\\nmight perform very well but state that it was trained by OpenAI or Meta\\ninstead of you. In this case, you might want to slightly steer the instruct\\nmodel‚Äôs behavior using preference alignment. By providing chosen and rejected\\nsamples for a small set of instructions (between 100 and 1000 samples), you\\ncan force the LLM to say that you trained it instead of OpenAI.\\n\\n## ‚öñÔ∏è SFT Techniques\\n\\nThe three most popular SFT techniques are full fine-tuning, LoRA, and QLoRA.\\n\\n**Full fine-tuning** is the most straightforward SFT technique. It involves\\nretraining all parameters of a pre-trained model on an instruction dataset.\\nThis method often provides the best results but requires significant\\ncomputational resources (several high-end GPUs are required to fine-tune a 8B\\nmodel). Because it modifies the entire model, it is also the most destructive\\nmethod and can lead to the catastrophic forgetting of previous skills and\\nknowledge.\\n\\n**Low-Rank Adaptation (LoRA)** is a popular parameter-efficient fine-tuning\\ntechnique. Instead of retraining the entire model, it freezes the weights and\\nintroduces small adapters (low-rank matrices) at each targeted layer. This\\nallows LoRA to train a number of parameters that is drastically lower than\\nfull fine-tuning (less than 1%), reducing both memory usage and training time.\\nThis method is non-destructive since the original parameters are frozen, and\\nadapters can then be switched or combined at will.\\n\\n**QLoRA (Quantization-aware Low-Rank Adaptation)** is an extension of LoRA\\nthat offers even greater memory savings. It provides up to 33% additional\\nmemory reduction compared to standard LoRA, making it particularly useful when\\nGPU memory is constrained. This increased efficiency comes at the cost of\\nlonger training times, with QLoRA typically taking about 39% more time to\\ntrain than regular LoRA.\\n\\nWhile QLoRA requires more training time, its substantial memory savings can\\nmake it the only viable option in scenarios where GPU memory is limited. For\\nthis reason, this is the technique we will use in the next section to fine-\\ntune a Llama 3.1 8B model on Google Colab.\\n\\n## ü¶ô Fine-Tune Llama 3.1 8B\\n\\nTo efficiently fine-tune a Llama 3.1 8B model, we‚Äôll use the Unsloth library\\nby Daniel and Michael Han. Thanks to its custom kernels, Unsloth provides 2x\\nfaster training and 60% memory use compared to other options, making it ideal\\nin a constrained environment like Colab. Unfortunately, Unsloth only supports\\nsingle-GPU settings at the moment. For multi-GPU settings, I recommend popular\\nalternatives like TRL and Axolotl (both also include Unsloth as a backend).\\n\\nIn this example, we will QLoRA fine-tune it on the mlabonne/FineTome-100k\\ndataset. It‚Äôs a subset of arcee-ai/The-Tome (without arcee-\\nai/qwen2-72b-magpie-en) that I re-filtered using HuggingFaceFW/fineweb-edu-\\nclassifier. Note that this classifier wasn‚Äôt designed for instruction data\\nquality evaluation, but we can use it as a rough proxy. The resulting FineTome\\nis an ultra-high quality dataset that includes conversations, reasoning\\nproblems, function calling, and more.\\n\\nLet‚Äôs start by installing all the required libraries.\\n\\n    \\n    \\n    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n    !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes __\\n\\nOnce installed, we can import them as follows.\\n\\n    \\n    \\n    import torch\\n    from trl import SFTTrainer\\n    from datasets import load_dataset\\n    from transformers import TrainingArguments, TextStreamer\\n    from unsloth.chat_templates import get_chat_template\\n    from unsloth import FastLanguageModel, is_bfloat16_supported __\\n\\nLet‚Äôs now load the model. Since we want to use QLoRA, I chose the pre-\\nquantized unsloth/Meta-Llama-3.1-8B-bnb-4bit. This 4-bit precision version of\\nmeta-llama/Meta-Llama-3.1-8B is significantly smaller (5.4 GB) and faster to\\ndownload compared to the original 16-bit precision model (16 GB). We load in\\nNF4 format using the bitsandbytes library.\\n\\nWhen loading the model, we must specify a maximum sequence length, which\\nrestricts its context window. Llama 3.1 supports up to 128k context length,\\nbut we will set it to 2,048 in this example since it consumes more compute and\\nVRAM. Finally, the `dtype` parameter automatically detects if your GPU\\nsupports the BF16 format for more stability during training (this feature is\\nrestricted to Ampere and more recent GPUs).\\n\\n    \\n    \\n    max_seq_length = 2048\\n    model, tokenizer = FastLanguageModel.from_pretrained(\\n        model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\\n        max_seq_length=max_seq_length,\\n        load_in_4bit=True,\\n        dtype=None,\\n    )__\\n\\nNow that our model is loaded in 4-bit precision, we want to prepare it for\\nparameter-efficient fine-tuning with LoRA adapters. LoRA has three important\\nparameters:\\n\\n  * **Rank** (r), which determines LoRA matrix size. Rank typically starts at 8 but can go up to 256. Higher ranks can store more information but increase the computational and memory cost of LoRA. We set it to 16 here.\\n  * **Alpha** (Œ±), a scaling factor for updates. Alpha directly impacts the adapters‚Äô contribution and is often set to 1x or 2x the rank value.\\n  * **Target modules** : LoRA can be applied to various model components, including attention mechanisms (Q, K, V matrices), output projections, feed-forward blocks, and linear output layers. While initially focused on attention mechanisms, extending LoRA to other components has shown benefits. However, adapting more modules increases the number of trainable parameters and memory needs.\\n\\nHere, we set r=16, Œ±=16, and target every linear module to maximize quality.\\nWe don‚Äôt use dropout and biases for faster training.\\n\\nIn addition, we will use Rank-Stabilized LoRA (rsLoRA), which modifies the\\nscaling factor of LoRA adapters to be proportional to 1/‚àör instead of 1/r.\\nThis stabilizes learning (especially for higher adapter ranks) and allows for\\nimproved fine-tuning performance as rank increases. Gradient checkpointing is\\nhandled by Unsloth to offload input and output embeddings to disk and save\\nVRAM.\\n\\n    \\n    \\n    model = FastLanguageModel.get_peft_model(\\n        model,\\n        r=16,\\n        lora_alpha=16,\\n        lora_dropout=0,\\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], \\n        use_rslora=True,\\n        use_gradient_checkpointing=\"unsloth\"\\n    )__\\n\\nWith this LoRA configuration, we‚Äôll only train 42 million out of 8 billion\\nparameters (0.5196%). This shows how much more efficient LoRA is compared to\\nfull fine-tuning.\\n\\nLet‚Äôs now load and prepare our dataset. Instruction datasets are stored in a\\n**particular format** : it can be Alpaca, ShareGPT, OpenAI, etc. First, we\\nwant to parse this format to retrieve our instructions and answers. Our\\nmlabonne/FineTome-100k dataset uses the ShareGPT format with a unique\\n‚Äúconversations‚Äù column containing messages in JSONL. Unlike simpler formats\\nlike Alpaca, ShareGPT is ideal for storing multi-turn conversations, which is\\ncloser to how users interact with LLMs.\\n\\nOnce our instruction-answer pairs are parsed, we want to reformat them to\\nfollow a **chat template**. Chat templates are a way to structure\\nconversations between users and models. They typically include special tokens\\nto identify the beginning and the end of a message, who‚Äôs speaking, etc. Base\\nmodels don‚Äôt have chat templates so we can choose any: ChatML, Llama3,\\nMistral, etc. In the open-source community, the ChatML template (originally\\nfrom OpenAI) is a popular option. It simply adds two special tokens\\n(`<|im_start|>` and `<|im_end|>`) to indicate who‚Äôs speaking.\\n\\nIf we apply this template to the previous instruction sample, here‚Äôs what we\\nget:\\n\\n    \\n    \\n    <|im_start|>system\\n    You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.<|im_end|>\\n    <|im_start|>user\\n    Remove the spaces from the following sentence: It prevents users to suspect that there are some hidden products installed on theirs device.\\n    <|im_end|>\\n    <|im_start|>assistant\\n    Itpreventsuserstosuspectthattherearesomehiddenproductsinstalledontheirsdevice.<|im_end|>\\n\\nIn the following code block, we parse our ShareGPT dataset with the `mapping`\\nparameter and include the ChatML template. We then load and process the entire\\ndataset to apply the chat template to every conversation.\\n\\n    \\n    \\n    tokenizer = get_chat_template(\\n        tokenizer,\\n        mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\\n        chat_template=\"chatml\",\\n    )\\n    \\n    def apply_template(examples):\\n        messages = examples[\"conversations\"]\\n        text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\\n        return {\"text\": text}\\n    \\n    dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\\n    dataset = dataset.map(apply_template, batched=True)__\\n\\nWe‚Äôre now ready to specify the training parameters for our run. I want to\\nbriefly introduce the most important hyperparameters:\\n\\n  * **Learning rate** : It controls how strongly the model updates its parameters. Too low, and training will be slow and may get stuck in local minima. Too high, and training may become unstable or diverge, which degrades performance.\\n  * **LR scheduler** : It adjusts the learning rate (LR) during training, starting with a higher LR for rapid initial progress and then decreasing it in later stages. Linear and cosine schedulers are the two most common options.\\n  * **Batch size** : Number of samples processed before the weights are updated. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, but they also require more memory. Gradient accumulation allows for effectively larger batch sizes by accumulating gradients over multiple forward/backward passes before updating the model.\\n  * **Num epochs** : The number of complete passes through the training dataset. More epochs allow the model to see the data more times, potentially leading to better performance. However, too many epochs can cause overfitting.\\n  * **Optimizer** : Algorithm used to adjust the parameters of a model to minimize the loss function. In practice, AdamW 8-bit is strongly recommended: it performs as well as the 32-bit version while using less GPU memory. The paged version of AdamW is only interesting in distributed settings.\\n  * **Weight decay** : A regularization technique that adds a penalty for large weights to the loss function. It helps prevent overfitting by encouraging the model to learn simpler, more generalizable features. However, too much weight decay can impede learning.\\n  * **Warmup steps** : A period at the beginning of training where the learning rate is gradually increased from a small value to the initial learning rate. Warmup can help stabilize early training, especially with large learning rates or batch sizes, by allowing the model to adjust to the data distribution before making large updates.\\n  * **Packing** : Batches have a pre-defined sequence length. Instead of assigning one batch per sample, we can combine multiple small samples in one batch, increasing efficiency.\\n\\nI trained the model on the entire dataset (100k samples) using an A100 GPU (40\\nGB of VRAM) on Google Colab. The training took 4 hours and 45 minutes. Of\\ncourse, you can use smaller GPUs with less VRAM and a smaller batch size, but\\nthey‚Äôre not nearly as fast. For example, it takes roughly 19 hours and 40\\nminutes on an L4 and a whopping 47 hours on a free T4.\\n\\nIn this case, I recommend only loading a subset of the dataset to speed up\\ntraining. You can do it by modifying the previous code block, like `dataset =\\nload_dataset(\"mlabonne/FineTome-100k\", split=\"train[:10000]\")` to only load\\n10k samples. Alternatively, you can use cheaper cloud GPU providers like\\nPaperspace, RunPod, or Lambda Labs.\\n\\n    \\n    \\n    trainer=SFTTrainer(\\n        model=model,\\n        tokenizer=tokenizer,\\n        train_dataset=dataset,\\n        dataset_text_field=\"text\",\\n        max_seq_length=max_seq_length,\\n        dataset_num_proc=2,\\n        packing=True,\\n        args=TrainingArguments(\\n            learning_rate=3e-4,\\n            lr_scheduler_type=\"linear\",\\n            per_device_train_batch_size=8,\\n            gradient_accumulation_steps=2,\\n            num_train_epochs=1,\\n            fp16=not is_bfloat16_supported(),\\n            bf16=is_bfloat16_supported(),\\n            logging_steps=1,\\n            optim=\"adamw_8bit\",\\n            weight_decay=0.01,\\n            warmup_steps=10,\\n            output_dir=\"output\",\\n            seed=0,\\n        ),\\n    )\\n    \\n    trainer.train()__\\n\\nNow that the model is trained, let‚Äôs test it with a simple prompt. This is not\\na rigorous evaluation but just a quick check to detect potential issues. We\\nuse `FastLanguageModel.for_inference()` to get 2x faster inference.\\n\\n    \\n    \\n    model = FastLanguageModel.for_inference(model)\\n    \\n    messages = [\\n        {\"from\": \"human\", \"value\": \"Is 9.11 larger than 9.9?\"},\\n    ]\\n    inputs = tokenizer.apply_chat_template(\\n        messages,\\n        tokenize=True,\\n        add_generation_prompt=True,\\n        return_tensors=\"pt\",\\n    ).to(\"cuda\")\\n    \\n    text_streamer = TextStreamer(tokenizer)\\n    _ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)__\\n\\nThe model‚Äôs response is ‚Äú9.9‚Äù, which is correct!\\n\\nLet‚Äôs now save our trained model. If you remember the part about LoRA and\\nQLoRA, what we trained is not the model itself but a set of adapters. There\\nare three save methods in Unsloth: `lora` to only save the adapters, and\\n`merged_16bit`/`merged_4bit` to merge the adapters with the model in 16-bit/\\n4-bit precision.\\n\\nIn the following, we merge them in 16-bit precision to maximize the quality.\\nWe first save it locally in the ‚Äúmodel‚Äù directory and then upload it to the\\nHugging Face Hub. You can find the trained model on mlabonne/FineLlama-3.1-8B.\\n\\n    \\n    \\n    model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\\n    model.push_to_hub_merged(\"mlabonne/FineLlama-3.1-8B\", tokenizer, save_method=\"merged_16bit\")__\\n\\nUnsloth also allows you to directly convert your model into GGUF format. This\\nis a quantization format created for llama.cpp and compatible with most\\ninference engines, like LM Studio, Ollama, and oobabooga‚Äôs text-generation-\\nwebui. Since you can specify different precisions (see my article about GGUF\\nand llama.cpp), we‚Äôll loop over a list to quantize it in `q2_k`, `q3_k_m`,\\n`q4_k_m`, `q5_k_m`, `q6_k`, `q8_0` and upload these quants on Hugging Face.\\nThe mlabonne/FineLlama-3.1-8B-GGUF contains all our GGUFs.\\n\\n    \\n    \\n    quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\\n    for quant in quant_methods:\\n        model.push_to_hub_gguf(\"mlabonne/FineLlama-3.1-8B-GGUF\", tokenizer, quant)__\\n\\nCongratulations, we fine-tuned a model from scratch and uploaded quants you\\ncan now use in your favorite inference engine. Feel free to try the final\\nmodel available on mlabonne/FineLlama-3.1-8B-GGUF. What to do now? Here are\\nsome ideas on how to use your model:\\n\\n  * **Evaluate** it on the Open LLM Leaderboard (you can submit it for free) or using other evals like in LLM AutoEval.\\n  * **Align** it with Direct Preference Optimization using a preference dataset like mlabonne/orpo-dpo-mix-40k to boost performance.\\n  * **Quantize** it in other formats like EXL2, AWQ, GPTQ, or HQQ for faster inference or lower precision using AutoQuant.\\n  * **Deploy** it on a Hugging Face Space with ZeroChat for models that have been sufficiently trained to follow a chat template (~20k samples).\\n\\n## Conclusion\\n\\nThis article provided a comprehensive overview of supervised fine-tuning and\\nhow to apply it in practice to a Llama 3.1 8B model. By leveraging QLoRA‚Äôs\\nefficient memory usage, we managed to fine-tune an 8B LLM on a super high-\\nquality dataset with limited GPU resources. We also provided more efficient\\nalternatives for bigger runs and suggestions for further steps, including\\nevaluation, preference alignment, quantization, and deployment.\\n\\nI hope this guide was useful. If you‚Äôre interested in learning more about\\nLLMs, I recommend checking the LLM Course. If you enjoyed this article, follow\\nme on X @maximelabonne and on Hugging Face @mlabonne. Good luck fine-tuning\\nmodels!\\n\\n__Copyright 2023, Maxime Labonne\\n\\n', 'language': 'en'} platform='mlabonne.github.io' author_id=UUID('eff74089-0271-4319-8543-745c087f4f61') author_full_name='Maxime Labonne' link='https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html'\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca691c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxime Labonne Fine tune Llama 3.1 Ultra Efficiently with Unsloth Maxime Labonne __LLM Course __Hands On GNNs __Research __About __ __ __ __ 1. LLM Post training 2. Fine tune Llama 3.1 8B 1. LLM Post training 2. Fine tune Llama 3.1 8B Fine tune Llama 3.1 Ultra Efficiently with Unsloth A beginner s guide to state of the art supervised fine tuning Large Language Models Author Maxime Lbonne Published July 29, 2024 LLM Post training __ Fine tune Llama 2 in Colab Fine tune Llama 2 in Axolotl Fine tune Mistral 7b with DPO Fine tune Llama 3 with ORPO Fine tune Llama 3.1 8B Merge LLMs with mergekit Create Mixture of Experts Uncensor any LLM LLM Quantization __ Intro to Quantization Quantization with GPTQ Quantization with GGML Quantization with ExLlamaV2 LLM stuff __ ChatGPT KG Decoding Strategies Agentic data generation Graph neural networks __ Graph Convolution Network Graph Attention Network GraphSAGE Graph Isomorphism Network Linear programming __ Linear Programming Integer Programming Constraint Programming Nonlinear Programming Miscellaneous __ Q learning Minecraft Bot Loops in Pandas What is a Tensor Sections Supervised Fine Tuning SFT Techniques Fine Tune Llama 3.1 8B Conclusion Pre order the LLM Engineer s Handbook , my new book to master the art of LLMs from concept to production The recent release of Llama 3.1 offers models with an incredible level of performance, closing the gap between closed source and open weight models. Instead of using frozen, general purpose LLMs like GPT 4o and Claude 3.5, you can fine tune Llama 3.1 for your specific use cases to achieve better performance and customizability at a lower cost. In this article, we will provide a comprehensive overview of supervised fine tuning. We will compare it to prompt engineering to understand when it makes sense to use it, detail the main techniques with their pros and cons, and introduce major concepts, such as LoRA hyperparameters, storage formats, and chat templates. Finally, we will implement it in practice by fine tuning Llama 3.1 8B in Google Colab with state of the art optimization using Unsloth. All the code used in this article is available on Google Colab and in the LLM Course. Special thanks to Daniel Han for answering my questions. Supervised Fine Tuning Supervised Fine Tuning SFT is a method to improve and customize pre trained LLMs. It involves retraining base models on a smaller dataset of instructions and answers. The main goal is to transform a basic model that predicts text into an assistant that can follow instructions and answer questions. SFT can also enhance the model s overall performance, add new knowledge, or adapt it to specific tasks and domains. Fine tuned models can then go through an optional preference alignment stage see my article about DPO to remove unwanted responses, modify their style, and more. The following figure shows an instruction sample. It includes a system prompt to steer the model, a user prompt to provide a task, and the output the model is expected to generate. You can find a list of high quality open source instruction datasets in the LLM Datasets GitHub repo. Before considering SFT, I recommend trying prompt engineering techniques like few shot prompting or retrieval augmented generation RAG . In practice, these methods can solve many problems without the need for fine tuning, using either closed source or open weight models e.g., Llama 3.1 Instruct . If this approach doesn t meet your objectives in terms of quality, cost, latency, etc. , then SFT becomes a viable option when instruction data is available. Note that SFT also offers benefits like additional control and customizability to create personalized LLMs. However, SFT has limitations. It works best when leveraging knowledge already present in the base model. Learning completely new information like an unknown language can be challenging and lead to more frequent hallucinations. For new domains unknown to the base model, it is recommended to continuously pre train it on a raw dataset first. On the opposite end of the spectrum, instruct models i.e., already fine tuned models can already be very close to your requirements. For example, a model might perform very well but state that it was trained by OpenAI or Meta instead of you. In this case, you might want to slightly steer the instruct model s behavior using preference alignment. By providing chosen and rejected samples for a small set of instructions between 100 and 1000 samples , you can force the LLM to say that you trained it instead of OpenAI. SFT Techniques The three most popular SFT techniques are full fine tuning, LoRA, and QLoRA. Full fine tuning is the most straightforward SFT technique. It involves retraining all parameters of a pre trained model on an instruction dataset. This method often provides the best results but requires significant computational resources several high end GPUs are required to fine tune a 8B model . Because it modifies the entire model, it is also the most destructive method and can lead to the catastrophic forgetting of previous skills and knowledge. Low Rank Adaptation LoRA is a popular parameter efficient fine tuning technique. Instead of retraining the entire model, it freezes the weights and introduces small adapters low rank matrices at each targeted layer. This allows LoRA to train a number of parameters that is drastically lower than full fine tuning less than 1 , reducing both memory usage and training time. This method is non destructive since the original parameters are frozen, and adapters can then be switched or combined at will. QLoRA Quantization aware Low Rank Adaptation is an extension of LoRA that offers even greater memory savings. It provides up to 33 additional memory reduction compared to standard LoRA, making it particularly useful when GPU memory is constrained. This increased efficiency comes at the cost of longer training times, with QLoRA typically taking about 39 more time to train than regular LoRA. While QLoRA requires more training time, its substantial memory savings can make it the only viable option in scenarios where GPU memory is limited. For this reason, this is the technique we will use in the next section to fine tune a Llama 3.1 8B model on Google Colab. Fine Tune Llama 3.1 8B To efficiently fine tune a Llama 3.1 8B model, we ll use the Unsloth library by Daniel and Michael Han. Thanks to its custom kernels, Unsloth provides 2x faster training and 60 memory use compared to other options, making it ideal in a constrained environment like Colab. Unfortunately, Unsloth only supports single GPU settings at the moment. For multi GPU settings, I recommend popular alternatives like TRL and Axolotl both also include Unsloth as a backend . In this example, we will QLoRA fine tune it on the mlabonne FineTome 100k dataset. It s a subset of arcee ai The Tome without arcee ai qwen2 72b magpie en that I re filtered using HuggingFaceFW fineweb edu classifier. Note that this classifier wasn t designed for instruction data quality evaluation, but we can use it as a rough proxy. The resulting FineTome is an ultra high quality dataset that includes conversations, reasoning problems, function calling, and more. Let s start by installing all the required libraries. !pip install unsloth colab new git https github.com unslothai unsloth.git !pip install no deps xformers 0.0.27 trl 0.9.0 peft accelerate bitsandbytes __ Once installed, we can import them as follows. import torch from trl import SFTTrainer from datasets import load_dataset from transformers import TrainingArguments, TextStreamer from unsloth.chat_templates import get_chat_template from unsloth import FastLanguageModel, is_bfloat16_supported __ Let s now load the model. Since we want to use QLoRA, I chose the pre quantized unsloth Meta Llama 3.1 8B bnb 4bit. This 4 bit precision version of meta llama Meta Llama 3.1 8B is significantly smaller 5.4 GB and faster to download compared to the original 16 bit precision model 16 GB . We load in NF4 format using the bitsandbytes library. When loading the model, we must specify a maximum sequence length, which restricts its context window. Llama 3.1 supports up to 128k context length, but we will set it to 2,048 in this example since it consumes more compute and VRAM. Finally, the dtype parameter automatically detects if your GPU supports the BF16 format for more stability during training this feature is restricted to Ampere and more recent GPUs . max_seq_length 2048 model, tokenizer FastLanguageModel.from_pretrained model_name unsloth Meta Llama 3.1 8B bnb 4bit , max_seq_length max_seq_length, load_in_4bit True, dtype None, __ Now that our model is loaded in 4 bit precision, we want to prepare it for parameter efficient fine tuning with LoRA adapters. LoRA has three important parameters Rank r , which determines LoRA matrix size. Rank typically starts at 8 but can go up to 256. Higher ranks can store more information but increase the computational and memory cost of LoRA. We set it to 16 here. Alpha Œ± , a scaling factor for updates. Alpha directly impacts the adapters contribution and is often set to 1x or 2x the rank value. Target modules LoRA can be applied to various model components, including attention mechanisms Q, K, V matrices , output projections, feed forward blocks, and linear output layers. While initially focused on attention mechanisms, extending LoRA to other components has shown benefits. However, adapting more modules increases the number of trainable parameters and memory needs. Here, we set r 16, Œ± 16, and target every linear module to maximize quality. We don t use dropout and biases for faster training. In addition, we will use Rank Stabilized LoRA rsLoRA , which modifies the scaling factor of LoRA adapters to be proportional to 1 r instead of 1 r. This stabilizes learning especially for higher adapter ranks and allows for improved fine tuning performance as rank increases. Gradient checkpointing is handled by Unsloth to offload input and output embeddings to disk and save VRAM. model FastLanguageModel.get_peft_model model, r 16, lora_alpha 16, lora_dropout 0, target_modules q_proj , k_proj , v_proj , up_proj , down_proj , o_proj , gate_proj , use_rslora True, use_gradient_checkpointing unsloth __ With this LoRA configuration, we ll only train 42 million out of 8 billion parameters 0.5196 . This shows how much more efficient LoRA is compared to full fine tuning. Let s now load and prepare our dataset. Instruction datasets are stored in a particular format it can be Alpaca, ShareGPT, OpenAI, etc. First, we want to parse this format to retrieve our instructions and answers. Our mlabonne FineTome 100k dataset uses the ShareGPT format with a unique conversations column containing messages in JSONL. Unlike simpler formats like Alpaca, ShareGPT is ideal for storing multi turn conversations, which is closer to how users interact with LLMs. Once our instruction answer pairs are parsed, we want to reformat them to follow a chat template . Chat templates are a way to structure conversations between users and models. They typically include special tokens to identify the beginning and the end of a message, who s speaking, etc. Base models don t have chat templates so we can choose any ChatML, Llama3, Mistral, etc. In the open source community, the ChatML template originally from OpenAI is a popular option. It simply adds two special tokens im_start and im_end to indicate who s speaking. If we apply this template to the previous instruction sample, here s what we get im_start system You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old. im_end im_start user Remove the spaces from the following sentence It prevents users to suspect that there are some hidden products installed on theirs device. im_end im_start assistant Itpreventsuserstosuspectthattherearesomehiddenproductsinstalledontheirsdevice. im_end In the following code block, we parse our ShareGPT dataset with the mapping parameter and include the ChatML template. We then load and process the entire dataset to apply the chat template to every conversation. tokenizer get_chat_template tokenizer, mapping role from , content value , user human , assistant gpt , chat_template chatml , def apply_template examples messages examples conversations text tokenizer.apply_chat_template message, tokenize False, add_generation_prompt False for message in messages return text text dataset load_dataset mlabonne FineTome 100k , split train dataset dataset.map apply_template, batched True __ We re now ready to specify the training parameters for our run. I want to briefly introduce the most important hyperparameters Learning rate It controls how strongly the model updates its parameters. Too low, and training will be slow and may get stuck in local minima. Too high, and training may become unstable or diverge, which degrades performance. LR scheduler It adjusts the learning rate LR during training, starting with a higher LR for rapid initial progress and then decreasing it in later stages. Linear and cosine schedulers are the two most common options. Batch size Number of samples processed before the weights are updated. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, but they also require more memory. Gradient accumulation allows for effectively larger batch sizes by accumulating gradients over multiple forward backward passes before updating the model. Num epochs The number of complete passes through the training dataset. More epochs allow the model to see the data more times, potentially leading to better performance. However, too many epochs can cause overfitting. Optimizer Algorithm used to adjust the parameters of a model to minimize the loss function. In practice, AdamW 8 bit is strongly recommended it performs as well as the 32 bit version while using less GPU memory. The paged version of AdamW is only interesting in distributed settings. Weight decay A regularization technique that adds a penalty for large weights to the loss function. It helps prevent overfitting by encouraging the model to learn simpler, more generalizable features. However, too much weight decay can impede learning. Warmup steps A period at the beginning of training where the learning rate is gradually increased from a small value to the initial learning rate. Warmup can help stabilize early training, especially with large learning rates or batch sizes, by allowing the model to adjust to the data distribution before making large updates. Packing Batches have a pre defined sequence length. Instead of assigning one batch per sample, we can combine multiple small samples in one batch, increasing efficiency. I trained the model on the entire dataset 100k samples using an A100 GPU 40 GB of VRAM on Google Colab. The training took 4 hours and 45 minutes. Of course, you can use smaller GPUs with less VRAM and a smaller batch size, but they re not nearly as fast. For example, it takes roughly 19 hours and 40 minutes on an L4 and a whopping 47 hours on a free T4. In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like dataset load_dataset mlabonne FineTome 100k , split train 10000 to only load 10k samples. Alternatively, you can use cheaper cloud GPU providers like Paperspace, RunPod, or Lambda Labs. trainer SFTTrainer model model, tokenizer tokenizer, train_dataset dataset, dataset_text_field text , max_seq_length max_seq_length, dataset_num_proc 2, packing True, args TrainingArguments learning_rate 3e 4, lr_scheduler_type linear , per_device_train_batch_size 8, gradient_accumulation_steps 2, num_train_epochs 1, fp16 not is_bfloat16_supported , bf16 is_bfloat16_supported , logging_steps 1, optim adamw_8bit , weight_decay 0.01, warmup_steps 10, output_dir output , seed 0, , trainer.train __ Now that the model is trained, let s test it with a simple prompt. This is not a rigorous evaluation but just a quick check to detect potential issues. We use FastLanguageModel.for_inference to get 2x faster inference. model FastLanguageModel.for_inference model messages from human , value Is 9.11 larger than 9.9? , inputs tokenizer.apply_chat_template messages, tokenize True, add_generation_prompt True, return_tensors pt , .to cuda text_streamer TextStreamer tokenizer _ model.generate input_ids inputs, streamer text_streamer, max_new_tokens 128, use_cache True __ The model s response is 9.9 , which is correct! Let s now save our trained model. If you remember the part about LoRA and QLoRA, what we trained is not the model itself but a set of adapters. There are three save methods in Unsloth lora to only save the adapters, and merged_16bit merged_4bit to merge the adapters with the model in 16 bit 4 bit precision. In the following, we merge them in 16 bit precision to maximize the quality. We first save it locally in the model directory and then upload it to the Hugging Face Hub. You can find the trained model on mlabonne FineLlama 3.1 8B. model.save_pretrained_merged model , tokenizer, save_method merged_16bit model.push_to_hub_merged mlabonne FineLlama 3.1 8B , tokenizer, save_method merged_16bit __ Unsloth also allows you to directly convert your model into GGUF format. This is a quantization format created for llama.cpp and compatible with most inference engines, like LM Studio, Ollama, and oobabooga s text generation webui. Since you can specify different precisions see my article about GGUF and llama.cpp , we ll loop over a list to quantize it in q2_k , q3_k_m , q4_k_m , q5_k_m , q6_k , q8_0 and upload these quants on Hugging Face. The mlabonne FineLlama 3.1 8B GGUF contains all our GGUFs. quant_methods q2_k , q3_k_m , q4_k_m , q5_k_m , q6_k , q8_0 for quant in quant_methods model.push_to_hub_gguf mlabonne FineLlama 3.1 8B GGUF , tokenizer, quant __ Congratulations, we fine tuned a model from scratch and uploaded quants you can now use in your favorite inference engine. Feel free to try the final model available on mlabonne FineLlama 3.1 8B GGUF. What to do now? Here are some ideas on how to use your model Evaluate it on the Open LLM Leaderboard you can submit it for free or using other evals like in LLM AutoEval. Align it with Direct Preference Optimization using a preference dataset like mlabonne orpo dpo mix 40k to boost performance. Quantize it in other formats like EXL2, AWQ, GPTQ, or HQQ for faster inference or lower precision using AutoQuant. Deploy it on a Hugging Face Space with ZeroChat for models that have been sufficiently trained to follow a chat template 20k samples . Conclusion This article provided a comprehensive overview of supervised fine tuning and how to apply it in practice to a Llama 3.1 8B model. By leveraging QLoRA s efficient memory usage, we managed to fine tune an 8B LLM on a super high quality dataset with limited GPU resources. We also provided more efficient alternatives for bigger runs and suggestions for further steps, including evaluation, preference alignment, quantization, and deployment. I hope this guide was useful. If you re interested in learning more about LLMs, I recommend checking the LLM Course. If you enjoyed this article, follow me on X maximelabonne and on Hugging Face mlabonne. Good luck fine tuning models! __Copyright 2023, Maxime Labonne en\n"
     ]
    }
   ],
   "source": [
    "from llmtwin.preprocess.clean_func import clean_text\n",
    "\n",
    "\n",
    "valid_content = [content for content in result.content.values() if content]\n",
    "print(clean_text('#### '.join(valid_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f00c9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtwin.clean import clean_documents\n",
    "\n",
    "cleaned_docs = clean_documents([result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f94c8094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CleanedArticle(id=UUID('c5de10b1-7cd2-4625-9a17-3359fba04afa'), content='Maxime Labonne Fine tune Llama 3.1 Ultra Efficiently with Unsloth Maxime Labonne __LLM Course __Hands On GNNs __Research __About __ __ __ __ 1. LLM Post training 2. Fine tune Llama 3.1 8B 1. LLM Post training 2. Fine tune Llama 3.1 8B Fine tune Llama 3.1 Ultra Efficiently with Unsloth A beginner s guide to state of the art supervised fine tuning Large Language Models Author Maxime Lbonne Published July 29, 2024 LLM Post training __ Fine tune Llama 2 in Colab Fine tune Llama 2 in Axolotl Fine tune Mistral 7b with DPO Fine tune Llama 3 with ORPO Fine tune Llama 3.1 8B Merge LLMs with mergekit Create Mixture of Experts Uncensor any LLM LLM Quantization __ Intro to Quantization Quantization with GPTQ Quantization with GGML Quantization with ExLlamaV2 LLM stuff __ ChatGPT KG Decoding Strategies Agentic data generation Graph neural networks __ Graph Convolution Network Graph Attention Network GraphSAGE Graph Isomorphism Network Linear programming __ Linear Programming Integer Programming Constraint Programming Nonlinear Programming Miscellaneous __ Q learning Minecraft Bot Loops in Pandas What is a Tensor Sections Supervised Fine Tuning SFT Techniques Fine Tune Llama 3.1 8B Conclusion Pre order the LLM Engineer s Handbook , my new book to master the art of LLMs from concept to production The recent release of Llama 3.1 offers models with an incredible level of performance, closing the gap between closed source and open weight models. Instead of using frozen, general purpose LLMs like GPT 4o and Claude 3.5, you can fine tune Llama 3.1 for your specific use cases to achieve better performance and customizability at a lower cost. In this article, we will provide a comprehensive overview of supervised fine tuning. We will compare it to prompt engineering to understand when it makes sense to use it, detail the main techniques with their pros and cons, and introduce major concepts, such as LoRA hyperparameters, storage formats, and chat templates. Finally, we will implement it in practice by fine tuning Llama 3.1 8B in Google Colab with state of the art optimization using Unsloth. All the code used in this article is available on Google Colab and in the LLM Course. Special thanks to Daniel Han for answering my questions. Supervised Fine Tuning Supervised Fine Tuning SFT is a method to improve and customize pre trained LLMs. It involves retraining base models on a smaller dataset of instructions and answers. The main goal is to transform a basic model that predicts text into an assistant that can follow instructions and answer questions. SFT can also enhance the model s overall performance, add new knowledge, or adapt it to specific tasks and domains. Fine tuned models can then go through an optional preference alignment stage see my article about DPO to remove unwanted responses, modify their style, and more. The following figure shows an instruction sample. It includes a system prompt to steer the model, a user prompt to provide a task, and the output the model is expected to generate. You can find a list of high quality open source instruction datasets in the LLM Datasets GitHub repo. Before considering SFT, I recommend trying prompt engineering techniques like few shot prompting or retrieval augmented generation RAG . In practice, these methods can solve many problems without the need for fine tuning, using either closed source or open weight models e.g., Llama 3.1 Instruct . If this approach doesn t meet your objectives in terms of quality, cost, latency, etc. , then SFT becomes a viable option when instruction data is available. Note that SFT also offers benefits like additional control and customizability to create personalized LLMs. However, SFT has limitations. It works best when leveraging knowledge already present in the base model. Learning completely new information like an unknown language can be challenging and lead to more frequent hallucinations. For new domains unknown to the base model, it is recommended to continuously pre train it on a raw dataset first. On the opposite end of the spectrum, instruct models i.e., already fine tuned models can already be very close to your requirements. For example, a model might perform very well but state that it was trained by OpenAI or Meta instead of you. In this case, you might want to slightly steer the instruct model s behavior using preference alignment. By providing chosen and rejected samples for a small set of instructions between 100 and 1000 samples , you can force the LLM to say that you trained it instead of OpenAI. SFT Techniques The three most popular SFT techniques are full fine tuning, LoRA, and QLoRA. Full fine tuning is the most straightforward SFT technique. It involves retraining all parameters of a pre trained model on an instruction dataset. This method often provides the best results but requires significant computational resources several high end GPUs are required to fine tune a 8B model . Because it modifies the entire model, it is also the most destructive method and can lead to the catastrophic forgetting of previous skills and knowledge. Low Rank Adaptation LoRA is a popular parameter efficient fine tuning technique. Instead of retraining the entire model, it freezes the weights and introduces small adapters low rank matrices at each targeted layer. This allows LoRA to train a number of parameters that is drastically lower than full fine tuning less than 1 , reducing both memory usage and training time. This method is non destructive since the original parameters are frozen, and adapters can then be switched or combined at will. QLoRA Quantization aware Low Rank Adaptation is an extension of LoRA that offers even greater memory savings. It provides up to 33 additional memory reduction compared to standard LoRA, making it particularly useful when GPU memory is constrained. This increased efficiency comes at the cost of longer training times, with QLoRA typically taking about 39 more time to train than regular LoRA. While QLoRA requires more training time, its substantial memory savings can make it the only viable option in scenarios where GPU memory is limited. For this reason, this is the technique we will use in the next section to fine tune a Llama 3.1 8B model on Google Colab. Fine Tune Llama 3.1 8B To efficiently fine tune a Llama 3.1 8B model, we ll use the Unsloth library by Daniel and Michael Han. Thanks to its custom kernels, Unsloth provides 2x faster training and 60 memory use compared to other options, making it ideal in a constrained environment like Colab. Unfortunately, Unsloth only supports single GPU settings at the moment. For multi GPU settings, I recommend popular alternatives like TRL and Axolotl both also include Unsloth as a backend . In this example, we will QLoRA fine tune it on the mlabonne FineTome 100k dataset. It s a subset of arcee ai The Tome without arcee ai qwen2 72b magpie en that I re filtered using HuggingFaceFW fineweb edu classifier. Note that this classifier wasn t designed for instruction data quality evaluation, but we can use it as a rough proxy. The resulting FineTome is an ultra high quality dataset that includes conversations, reasoning problems, function calling, and more. Let s start by installing all the required libraries. !pip install unsloth colab new git https github.com unslothai unsloth.git !pip install no deps xformers 0.0.27 trl 0.9.0 peft accelerate bitsandbytes __ Once installed, we can import them as follows. import torch from trl import SFTTrainer from datasets import load_dataset from transformers import TrainingArguments, TextStreamer from unsloth.chat_templates import get_chat_template from unsloth import FastLanguageModel, is_bfloat16_supported __ Let s now load the model. Since we want to use QLoRA, I chose the pre quantized unsloth Meta Llama 3.1 8B bnb 4bit. This 4 bit precision version of meta llama Meta Llama 3.1 8B is significantly smaller 5.4 GB and faster to download compared to the original 16 bit precision model 16 GB . We load in NF4 format using the bitsandbytes library. When loading the model, we must specify a maximum sequence length, which restricts its context window. Llama 3.1 supports up to 128k context length, but we will set it to 2,048 in this example since it consumes more compute and VRAM. Finally, the dtype parameter automatically detects if your GPU supports the BF16 format for more stability during training this feature is restricted to Ampere and more recent GPUs . max_seq_length 2048 model, tokenizer FastLanguageModel.from_pretrained model_name unsloth Meta Llama 3.1 8B bnb 4bit , max_seq_length max_seq_length, load_in_4bit True, dtype None, __ Now that our model is loaded in 4 bit precision, we want to prepare it for parameter efficient fine tuning with LoRA adapters. LoRA has three important parameters Rank r , which determines LoRA matrix size. Rank typically starts at 8 but can go up to 256. Higher ranks can store more information but increase the computational and memory cost of LoRA. We set it to 16 here. Alpha Œ± , a scaling factor for updates. Alpha directly impacts the adapters contribution and is often set to 1x or 2x the rank value. Target modules LoRA can be applied to various model components, including attention mechanisms Q, K, V matrices , output projections, feed forward blocks, and linear output layers. While initially focused on attention mechanisms, extending LoRA to other components has shown benefits. However, adapting more modules increases the number of trainable parameters and memory needs. Here, we set r 16, Œ± 16, and target every linear module to maximize quality. We don t use dropout and biases for faster training. In addition, we will use Rank Stabilized LoRA rsLoRA , which modifies the scaling factor of LoRA adapters to be proportional to 1 r instead of 1 r. This stabilizes learning especially for higher adapter ranks and allows for improved fine tuning performance as rank increases. Gradient checkpointing is handled by Unsloth to offload input and output embeddings to disk and save VRAM. model FastLanguageModel.get_peft_model model, r 16, lora_alpha 16, lora_dropout 0, target_modules q_proj , k_proj , v_proj , up_proj , down_proj , o_proj , gate_proj , use_rslora True, use_gradient_checkpointing unsloth __ With this LoRA configuration, we ll only train 42 million out of 8 billion parameters 0.5196 . This shows how much more efficient LoRA is compared to full fine tuning. Let s now load and prepare our dataset. Instruction datasets are stored in a particular format it can be Alpaca, ShareGPT, OpenAI, etc. First, we want to parse this format to retrieve our instructions and answers. Our mlabonne FineTome 100k dataset uses the ShareGPT format with a unique conversations column containing messages in JSONL. Unlike simpler formats like Alpaca, ShareGPT is ideal for storing multi turn conversations, which is closer to how users interact with LLMs. Once our instruction answer pairs are parsed, we want to reformat them to follow a chat template . Chat templates are a way to structure conversations between users and models. They typically include special tokens to identify the beginning and the end of a message, who s speaking, etc. Base models don t have chat templates so we can choose any ChatML, Llama3, Mistral, etc. In the open source community, the ChatML template originally from OpenAI is a popular option. It simply adds two special tokens im_start and im_end to indicate who s speaking. If we apply this template to the previous instruction sample, here s what we get im_start system You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old. im_end im_start user Remove the spaces from the following sentence It prevents users to suspect that there are some hidden products installed on theirs device. im_end im_start assistant Itpreventsuserstosuspectthattherearesomehiddenproductsinstalledontheirsdevice. im_end In the following code block, we parse our ShareGPT dataset with the mapping parameter and include the ChatML template. We then load and process the entire dataset to apply the chat template to every conversation. tokenizer get_chat_template tokenizer, mapping role from , content value , user human , assistant gpt , chat_template chatml , def apply_template examples messages examples conversations text tokenizer.apply_chat_template message, tokenize False, add_generation_prompt False for message in messages return text text dataset load_dataset mlabonne FineTome 100k , split train dataset dataset.map apply_template, batched True __ We re now ready to specify the training parameters for our run. I want to briefly introduce the most important hyperparameters Learning rate It controls how strongly the model updates its parameters. Too low, and training will be slow and may get stuck in local minima. Too high, and training may become unstable or diverge, which degrades performance. LR scheduler It adjusts the learning rate LR during training, starting with a higher LR for rapid initial progress and then decreasing it in later stages. Linear and cosine schedulers are the two most common options. Batch size Number of samples processed before the weights are updated. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, but they also require more memory. Gradient accumulation allows for effectively larger batch sizes by accumulating gradients over multiple forward backward passes before updating the model. Num epochs The number of complete passes through the training dataset. More epochs allow the model to see the data more times, potentially leading to better performance. However, too many epochs can cause overfitting. Optimizer Algorithm used to adjust the parameters of a model to minimize the loss function. In practice, AdamW 8 bit is strongly recommended it performs as well as the 32 bit version while using less GPU memory. The paged version of AdamW is only interesting in distributed settings. Weight decay A regularization technique that adds a penalty for large weights to the loss function. It helps prevent overfitting by encouraging the model to learn simpler, more generalizable features. However, too much weight decay can impede learning. Warmup steps A period at the beginning of training where the learning rate is gradually increased from a small value to the initial learning rate. Warmup can help stabilize early training, especially with large learning rates or batch sizes, by allowing the model to adjust to the data distribution before making large updates. Packing Batches have a pre defined sequence length. Instead of assigning one batch per sample, we can combine multiple small samples in one batch, increasing efficiency. I trained the model on the entire dataset 100k samples using an A100 GPU 40 GB of VRAM on Google Colab. The training took 4 hours and 45 minutes. Of course, you can use smaller GPUs with less VRAM and a smaller batch size, but they re not nearly as fast. For example, it takes roughly 19 hours and 40 minutes on an L4 and a whopping 47 hours on a free T4. In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like dataset load_dataset mlabonne FineTome 100k , split train 10000 to only load 10k samples. Alternatively, you can use cheaper cloud GPU providers like Paperspace, RunPod, or Lambda Labs. trainer SFTTrainer model model, tokenizer tokenizer, train_dataset dataset, dataset_text_field text , max_seq_length max_seq_length, dataset_num_proc 2, packing True, args TrainingArguments learning_rate 3e 4, lr_scheduler_type linear , per_device_train_batch_size 8, gradient_accumulation_steps 2, num_train_epochs 1, fp16 not is_bfloat16_supported , bf16 is_bfloat16_supported , logging_steps 1, optim adamw_8bit , weight_decay 0.01, warmup_steps 10, output_dir output , seed 0, , trainer.train __ Now that the model is trained, let s test it with a simple prompt. This is not a rigorous evaluation but just a quick check to detect potential issues. We use FastLanguageModel.for_inference to get 2x faster inference. model FastLanguageModel.for_inference model messages from human , value Is 9.11 larger than 9.9? , inputs tokenizer.apply_chat_template messages, tokenize True, add_generation_prompt True, return_tensors pt , .to cuda text_streamer TextStreamer tokenizer _ model.generate input_ids inputs, streamer text_streamer, max_new_tokens 128, use_cache True __ The model s response is 9.9 , which is correct! Let s now save our trained model. If you remember the part about LoRA and QLoRA, what we trained is not the model itself but a set of adapters. There are three save methods in Unsloth lora to only save the adapters, and merged_16bit merged_4bit to merge the adapters with the model in 16 bit 4 bit precision. In the following, we merge them in 16 bit precision to maximize the quality. We first save it locally in the model directory and then upload it to the Hugging Face Hub. You can find the trained model on mlabonne FineLlama 3.1 8B. model.save_pretrained_merged model , tokenizer, save_method merged_16bit model.push_to_hub_merged mlabonne FineLlama 3.1 8B , tokenizer, save_method merged_16bit __ Unsloth also allows you to directly convert your model into GGUF format. This is a quantization format created for llama.cpp and compatible with most inference engines, like LM Studio, Ollama, and oobabooga s text generation webui. Since you can specify different precisions see my article about GGUF and llama.cpp , we ll loop over a list to quantize it in q2_k , q3_k_m , q4_k_m , q5_k_m , q6_k , q8_0 and upload these quants on Hugging Face. The mlabonne FineLlama 3.1 8B GGUF contains all our GGUFs. quant_methods q2_k , q3_k_m , q4_k_m , q5_k_m , q6_k , q8_0 for quant in quant_methods model.push_to_hub_gguf mlabonne FineLlama 3.1 8B GGUF , tokenizer, quant __ Congratulations, we fine tuned a model from scratch and uploaded quants you can now use in your favorite inference engine. Feel free to try the final model available on mlabonne FineLlama 3.1 8B GGUF. What to do now? Here are some ideas on how to use your model Evaluate it on the Open LLM Leaderboard you can submit it for free or using other evals like in LLM AutoEval. Align it with Direct Preference Optimization using a preference dataset like mlabonne orpo dpo mix 40k to boost performance. Quantize it in other formats like EXL2, AWQ, GPTQ, or HQQ for faster inference or lower precision using AutoQuant. Deploy it on a Hugging Face Space with ZeroChat for models that have been sufficiently trained to follow a chat template 20k samples . Conclusion This article provided a comprehensive overview of supervised fine tuning and how to apply it in practice to a Llama 3.1 8B model. By leveraging QLoRA s efficient memory usage, we managed to fine tune an 8B LLM on a super high quality dataset with limited GPU resources. We also provided more efficient alternatives for bigger runs and suggestions for further steps, including evaluation, preference alignment, quantization, and deployment. I hope this guide was useful. If you re interested in learning more about LLMs, I recommend checking the LLM Course. If you enjoyed this article, follow me on X maximelabonne and on Hugging Face mlabonne. Good luck fine tuning models! __Copyright 2023, Maxime Labonne en', platform='mlabonne.github.io', author_id=UUID('eff74089-0271-4319-8543-745c087f4f61'), author_full_name='Maxime Labonne', link='https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtwin.domain.cleaned_document import CleanedArticle\n",
    "\n",
    "CleanedArticle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmtwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
