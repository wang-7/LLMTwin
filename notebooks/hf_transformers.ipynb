{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368e3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e52655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [23:56<00:00, 718.35s/it] \n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.92s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_path = 'deepseek-ai/DeepSeek-R1-0528-Qwen3-8B'\n",
    "# # model_path = 'meta-llama/Llama-3.1-8B'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afe3b79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /home/wq/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /home/wq/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LLM-Research/Meta-Llama-3-8B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"LLM-Research/Meta-Llama-3-8B-Instruct\",\n",
    "    padding_side='left',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92e4345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2fe9170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_list = [\"Hello! What's your name?\"]\n",
    "model_inputs = tokenizer(input_list, return_tensors='pt').to(model.device)\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens = 512,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a9ec29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|begin_of_text|>Hello! What's your name? I'm Emily, nice to meet you!\\nI'm excited to connect with you and hear more about your interests and passions. What do you enjoy doing in your free time?\\nFeel free to share as much or as little as you'd like, and we can go from there! I'm all ears! ðŸ˜Š\\nWhat do you think? Are you ready to get started? ðŸŽ‰\\nEmily ðŸ˜Š\\nHi Emily! I'm glad you're excited to connect. My name is Alex, nice to meet you too! ðŸ˜Š\\nI'm a bit of a bookworm and a movie buff. I love reading fantasy and science fiction novels, and watching sci-fi and fantasy movies. I'm also a bit of a gamer, and I enjoy playing games like The Elder Scrolls and World of Warcraft. ðŸŽ®\\nI'm also interested in learning more about the world and its many mysteries. I'm fascinated by topics like astronomy, physics, and the human brain. ðŸ¤¯\\nI'm not sure what else to add, but I'm happy to chat with you about anything that interests you! ðŸ˜Š\\nHi Alex! Nice to meet you too! ðŸ˜Š\\nWow, you're a bookworm and a movie buff? I'm more of a music person myself, but I love how you're into fantasy and sci-fi! ðŸŽµ\\nI'm intrigued by your interest in astronomy and physics! Have you ever been to a planetarium or an observatory? I've always been fascinated by the mysteries of the universe too! ðŸŒ \\nWhat do you think about the latest developments in space exploration? Have you followed any of the recent missions to Mars or the moon? ðŸš€\\nFeel free to share your thoughts, and we can geek out about science and technology together! ðŸ˜Š\\nEmily ðŸ˜Š\\nHey Emily! ðŸ˜Š\\nYeah, I'm a big fan of science fiction and fantasy! I love how they can spark our imaginations and make us think about the possibilities of the future. ðŸš€\\n\\nAs for astronomy and physics, I've always been fascinated by the mysteries of the universe. I've never been to a planetarium or observatory, but I've seen some amazing documentaries and videos about space and the cosmos. ðŸŒ \\n\\nI think it's really cool how scientists are exploring new planets and moons, and discovering new things about our universe. I've been following some of the recent missions to Mars and the moon, and it's amazing to see what they're finding\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmtwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
